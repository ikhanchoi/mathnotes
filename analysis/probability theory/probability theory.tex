\documentclass{../../large}
\usepackage{../../ikhanchoi}


\newcommand{\Unif}{\mathrm{Unif}}
\newcommand{\Bern}{\mathrm{Bern}}
\newcommand{\Binom}{\mathrm{Binom}}
\newcommand{\Geo}{\mathrm{Geo}}
\newcommand{\Exp}{\mathrm{Exp}}
\newcommand{\Pois}{\mathrm{Pois}}



\begin{document}
\title{Probability Theory}
\author{Ikhan Choi}
\maketitle
\tableofcontents


\part{Probability distributions}


\chapter{Random variables}

\section{Probability distributions}

\begin{prb}[Sample space]
Mathematically, a \emph{sample space} is defined as a measure space $(\Omega,\cF,P)$ with $P(\Omega)=1$.
Elements and measurable subsets of a sample space are called \emph{outcomes} and \emph{events}, respectively.
Let $\Omega$ be a fixed sample space.
Then, a \emph{random element} is a measurable function $X:\Omega\to S$ to a measurable space $S$, called the \emph{state space}. 
If the state space $S$ is the set of real numbers $\R$ together with the Borel $\sigma$-algebra, we call the random element $X$ as a \emph{random variable}.

Consider a statistical study of ages of people in the earth.
For the study, we set the \emph{population} $\cP=\{\text{ people in the earth }\}$ and the age function $a:\cP\to\Z_{\ge0}$.
In probability theory and statistics, we are interested in the estimation of the size of $a^{-1}(k)$ for each $k\in\Z_{\ge0}$, not in the exact description of the age function $a$.

Let us say that we conducted an experiment in which $n$ people are randomly chosen with replacement, to verify a hypothesis.
If we denote by $p_i$ the $i$th person, then 

Then a reasonable choice for the domain of the functions $X_i$ is $\Omega=\cP^n$.

Believing the fatalism, an experiment can be seen as a process of revealing a pre-determined fate $\omega$, which is what we call sample or outcome.


\begin{parts}
\item 
\end{parts}
\end{prb}
\begin{prb}[Probability distribution]
Let $X:\Omega\to\R$ be a random variable.
The (probability) \emph{distribution} of $X$ is the pushforward measure $X_*P$ on $\R$.
The right continuous increasing function $F$ corresponded to $X_*P$ is called the (cumulative) \emph{distribution function}.

If the distribution has discrete support, then we say $X$ is \emph{discrete}.
Since a probability measure of discrete support is a countable convex combination of Dirac measures, we can define the (probability) \emph{mass function} $p:\supp(X_*P)\to[0,1]$.
If the distribution is absolutely continuous with respect to the Lebesgue measure, then we say $X$ is \emph{continuous}.
By the Radon-Nikodym theorem, we can define the (probability) \emph{density function} $f\in L^1(\R)$.
The mass and density functions are effective ways to describe distributions of random variables in most applications.
\begin{parts}
\item
\end{parts}
\end{prb}
\begin{prb}[Expectation and moments]
Chebyshev's inequality
\end{prb}
\begin{prb}[Joint distribution]
\end{prb}
\begin{prb}[Distribution of functions]
transformation, function
\end{prb}

\section{Discrete distributions}

\section{Continuous distributions}

\section{Independence}

\begin{prb}[Dynkin's $\pi$-$\lambda$ lemma]
Let $\cP$ be a $\pi$-system and $\cL$ a $\lambda$-system respectively.
Denote by $\ell(\cP)$ the smallest $\lambda$-system containing $\cP$.
\begin{parts}
\item If $A\in\ell(\cP)$, then $\cG_A:=\{B:A\cap B\in\ell(\cP)\}$ is a $\lambda$-system.
\item $\ell(\cP)$ is a $\pi$-system.
\item If a $\lambda$-system is a $\pi$-system, then it is a $\sigma$-algebra.
\item If $\cP\subset\cL$, then $\sigma(\cP)\subset\cL$.
\end{parts}
\end{prb}

\begin{prb}[Monotone class lemma]

\end{prb}





\section*{Exercises}

equally likely outcomes
	coin toss
	dice roll
	ball drawing
	number permutation
	life time of a light bulb






\chapter{Conditional probablity}

\section*{Exercises}

\begin{prb}[Monty Hall problem]
Suppose you are on a game show, and given the choice of three doors $A$, $B$, and $C$.
Behind one door is a car; behind the others, goats.
You know that the probabilities $a$, $b$, and $c=1-a-b$.
You pick a door, say $A$, and the host, who knows what's behind the doors, opens another door, say $B$, which has a goat.
He then says to you, ``Do you want to pick door $C$?''
Is it to your advantage to switch your choice?
\begin{parts}
\item Find the condition for $a,b,c$ that the participant benefits when changed the choice.
\end{parts}
\end{prb}
\begin{pf}
Let $A$, $B$, and $C$ be the events that a car is behind the doors $A$, $B$, and $C$, respectively.
Let $X$ the event that the game host opened $B$.
Note $\{A,B,C\}$ is a partition of the sample space $\Omega$, and $X$ is independent to $A$, $B$, and $C$.
Then, $P(A)=P(B)=P(C)=1/3$, and
\[P(X|A)=\frac12,\quad P(X|B)=0,\quad P(X|C)=1.\]
Therefore,
\begin{align*}
P(C|X)&=\frac{P(X\cap C)}{P(X)}\\
&=\frac{P(X|C)P(C)}{P(X|A)P(A)+P(X|B)P(B)+P(X|C)P(C)}\\
&=\frac{1\cdot \frac13}{\frac12\cdot\frac13+0\cdot\frac13+1\cdot\frac13}=\frac23.
\end{align*}
Similarly, $P(A|X)=\frac13$ and $P(B|X)=0$.
\end{pf}










\chapter{Convergence of probability measures}


\section{Weak convergence in $\R$}
\begin{prb}[Portemanteau theorem]
Let $F_n$ and $F$ be distribution functions $\R\to[0,1]$.
We will define the \emph{weak convergence} as follows: $F_n$ converges weakly to $F$ if $F_n(x)\to F(x)$ for every continuity point $x$ of $F(x)$.
\begin{parts}
\item $F_n(x)\to F(x)$ for all continuity points $x$ of $F$.
\end{parts}
\end{prb}

\begin{prb}[Skorokhod representation theorem]
\end{prb}

\begin{prb}[Continuous mapping theorem]
\end{prb}

\begin{prb}[Slutsky's theorem]
\end{prb}

\begin{prb}[Helly's selection theorem]
\begin{parts}
\item Monotonically increasing functions $F_n:\R\to[0,1]$ has a pointwise convergent subsequence.
\item If $(F_n)_n$ is tight, then
\end{parts}
\end{prb}






\begin{prb}[Properties of probability Borel measures]
Let $S$ be a topological space.
\begin{parts}
\item Every single probability Borel measure is regular if $S$ is perfectly normal. (inner approximateion by closed sets)
\item Every single probability Borel measure is tight if $S$ is Polish. (inner approximation by compact sets)
\end{parts}
\end{prb}





\section{Weak topology in the space of probability measures}


\begin{prb}[Local limit theorems]
Suppose $f_n$ and $f$ are density functions.
\begin{parts}
\item If $f_n\to f$ a.s., then $f_n\to f$ in $L^1$.\hfill(Scheff\'e's theorem)
\item $f_n\to f$ in $L^1$ if and only if in total variation.
\item If $f_n\to f$ in total variation, then $f_n\to f$ weakly.
\end{parts}
\end{prb}


% pi system that generates sigma alg is separating
% in \R^\infty and C(I), set of finite dimensional sets is separating




\begin{prb}[Portmanteau theorem]
Let $S$ be a normal space and, $\mu_\alpha$ be a net in $\Prob(S)$.
We define the \emph{weak convergence} as follows: $\mu_\alpha$ converges weakly to $\mu$ if
\[\int f\,d\mu_\alpha\to\int f\,d\mu\]
for every $f\in C_b(S)$.
The following statements are all equivalent.
\begin{parts}
\item $\mu_\alpha\Rightarrow\mu$
\item $\mu_\alpha(g)\to\mu(g)$ for every uniformly continuous $g\in C_b(S)$.
\item $\limsup_\alpha\mu_\alpha(F)\le\mu(F)$ for every closed $F$.
\item $\liminf_\alpha\mu_\alpha(U)\ge\mu(U)$ for every open $U$.
\item $\lim_\alpha\mu_\alpha(A)=\mu(A)$ for every Borel $A$ such that $\mu(\partial A)=0$.
\end{parts}
\end{prb}
\begin{pf}
(a)$\Rightarrow$(b)
Clear.

(b)$\Rightarrow$(c)
Let $U$ be an open set such that $F\subset U$.
There is uniformly continuous $g\in C_b(S)$ such that $\1_F\le g\le\1_U$.
Therefore,
\[\limsup_\alpha\mu_\alpha(F)\le\limsup_\alpha\mu_\alpha(g)=\mu(g)\le\mu(U).\]
By the outer regularity of $\mu$, we obtain $\limsup_\alpha\mu_\alpha(F)\le\mu(F)$.

(c)$\Leftrightarrow$(d)
Clear.

(c)$+$(d)$\Rightarrow$(e)
It easily follows from
\[\limsup_\alpha\mu_\alpha(\bar A)\le\mu(\bar A)=\mu(A)=\mu(A^\circ)\le\liminf_\alpha\mu_\alpha(A^\circ).\]

(e)$\Rightarrow$(a)
Let $g\in C_b(S)$ and $\e>0$.
Since the pushforward measrue $g_*\mu$ has at most countably many mass points, there is a partition $(t_i)_{i=0}^n$ of an interval containing $[-\|g\|,\|g\|]$ such that $|t_{i+1}-t_i|<\e$ and $\mu(\{x:g(x)=t_i\})=0$ for each $i$.
Let $(A_i)_{i=0}^{n-1}$ be a Borel decomposition of $S$ given by $A_i:=g^{-1}([t_i,t_{i+1}))$, and define $f_\e:=\sum_{i=0}^{n-1}t_i\1_{A_i}$ so that we have $\sup_{x\in S}|g_\e(x)-g(x)|\le\e$.
From
\begin{align*}
|\mu_\alpha(g)-\mu(g)|&\le|\mu_\alpha(g-g_\e)|+|\mu_\alpha(g_\e)-\mu(g_\e)|+|\mu(g_\e-g)|\\
&\le\e+\sum_{i=0}^{n-1}|t_i||\mu_\alpha(A_i)-\mu(A_i)|+\e,
\end{align*}
we get
\[\limsup_\alpha|\mu_\alpha(g)-\mu(g)|<2\e.\]
Since $\e$ is arbitrary, we are done.
\end{pf}



\begin{prb}[Embedding by Dirac measures]
Let $S$ be a normal space.
\begin{parts}
\item $S\to\Prob(S)$ is an embedding.
\item $S\subset\Prob(S)$ is sequentially closed.
\item % If a metric is given, then completeness?
\end{parts}
\end{prb}
\begin{pf}
(a)
It uses Urysohn.

(b)
It uses (b)=>(c) of Portmanteau.
\end{pf}


\begin{prb}[L\'evy-Prokhorov metric]
Let $S$ be a metric space, and $\Prob(S)$ be the set of probability (regular) Borel measures on $S$.
Define $\pi:\Prob(S)\times\Prob(S)\to[0,\infty)$ such that
\[\pi(\mu,\nu):=\inf\{\,\alpha>0:\mu(A)\le\nu(A^\alpha)+\alpha,\ \nu(A)\le\mu(A^\alpha)+\alpha,\ \forall A\in\cB(S)\,\},\]
where $A^\alpha$ is the $\alpha$-neighborhood of $a$.
\begin{parts}
\item $\pi$ is a metric.
\item $\mu_n\to\mu$ in $\pi$ implies $\mu_n\Rightarrow\mu$.
\item $\mu_\alpha\Rightarrow\mu$ implies $\mu_\alpha\to\mu$ in $\pi$, if $S$ is separable.
\item $(S,d)$ is separable if and only if $(\Prob(S),\pi)$ is separable.
\item $(S,d)$ is compact if and only if $(\Prob(S),\pi)$ is compact
\item $(S,d)$ is complete if and only if $(\Prob(S),\pi)$ is complete.
\end{parts}
\end{prb}
\begin{pf}
(c)
\end{pf}




\begin{prb}[Direct direction of Prokhorov's theorem]
Let $S$ be a Polish space.
Let $\Prob(S)$ be the space of probability measures on $S$ endowed with the topology of weak convergence.
Prokhorov's theorem states that a subset of $\Prob(S)$ is relatively compact if and only if it is tight.
We prove one direction, in which the construction of a sufficiently large compact set is a main issue.

Let $\mu\in\Prob(S)$ and let $M$ be a relatively compact subset of $\Prob(S)$.
\begin{parts}
\item Every open cover $\{B_\alpha\}_\alpha$ of $S$ has a finite subcollection $\{B_i\}_i$ for each $\e>0$ such that
\[\mu\Bigl(\bigcup_iB_i\Bigr)>1-\e.\]
\item Every open cover $\{B_\alpha\}_\alpha$ of $S$ has a finite subcollection $\{B_i\}_i$ for each $\e>0$ such that
\[\inf_{\mu\in M}\mu\Bigl(\bigcup_iB_i\Bigr)>1-\e.\]
\item $M$ is tight: there is a compact $K\subset S$ for each $\e>0$ such that
\[\inf_{\mu\in M}\mu(K)>1-\e.\]
\end{parts}
\end{prb}
\begin{pf}
(a)
Since a separable metric space is Lindel\"of, we may assume $\{B_\alpha\}_\alpha=\{B_i\}_{i=1}^\infty$ is countable.
Then, we can deduce the conclusion from the continuity from below and the fact $\mu_0(S)=1$.

(b)
Suppose that the conclusion is not true so that there are $\e>0$ and a sequence $\mu_n\in M$ such that
\[\mu_n\Bigl(\bigcup_{i=1}^nB_i\Bigr)\le1-\e.\]
If we take a subsequence $(\mu_{n_k})_k$ that converges weakly to $\mu\in\bar M$ using the compactness of $\bar M$, then by the Portmanteau theorem we have for any $n$ that
\[\mu\Bigl(\bigcup_{i=1}^nB_i\Bigr)\le\liminf_{k\to\infty}\mu_{n_k}\Bigl(\bigcup_{i=1}^nB_i\Bigr)\le\liminf_{k\to\infty}\mu_{n_k}\Bigl(\bigcup_{i=1}^{n_k}B_i\Bigr)\le1-\e.\]
By taking $n$ sufficiently large, we lead a contradiction to the part (a).

(c)
Here we need metrizability, which leads to the exitence of countable fundamental system of uniformity for $\frac\e{2^m}$ argument.
Also we need the completeness to change the total boundedness to compactness.

Let $\{x_i\}_{i=1}^\infty$ be a dense set in $S$.
Then, since $\{B(x_i,\frac1m)\}_{i=1}^\infty$ is a countable open cover of $S$ for each integer $m>0$, there is a finite $n_m>0$ such that
\[\inf_{\mu\in M}\mu\Bigl(\bigcup_{i=1}^{n_m}B(x_i,\tfrac1m)\Bigr)>1-\frac\e{2^m}.\]
Define
\[K:=\bigcap_{m=1}^\infty\bigcup_{i=1}^{n_m}\bar{B(x_i,\tfrac1m)}.\]
It is closed and totally bounded in a complete metric space $S$, so $K$ is compact.
Moreover, we can verify
\[1-\mu(K)=\mu\Bigl(\bigcup_{m=1}^\infty\bigcap_{i=1}^{n_m}\bar{B(x_i,\tfrac1m)}^c\Bigr)\le\sum_{m=1}^\infty\left(1-\mu\Bigl(\bigcup_{i=1}^{n_m}B(x_i,\tfrac1m)\Bigr)\right)<\e\]
for every $\mu\in M$, so $M$ is tight.
\end{pf}

\begin{prb}[Converse direction of Prokhorov's theorem]
The ``converse'' direction of Prokhorov's theorem is related to a construction of measure and considered to be more difficult.
However, it holds in a general setting.

Let $S$ be a normal space.
Let $\Prob(S)$ be the space of probability measures on $S$ endowed with the topology of weak convergence.
Let $M$ be a tight subset of $\Prob(S)$ and let $(\mu_\alpha)_\alpha\subset M$ be a net.
We want to show that it has a convergent subnet in $\Prob(S)$.
\begin{parts}
\item $M$ is relatively compact.
\end{parts}
\end{prb}
\begin{pf}
Let $\beta S$ be the Stone-\v Cech compactification of $S$.
The inclusion $\iota:S\to\beta S$ is a topological embedding because $S$ is completely regular.
Pushforward the measures $\mu_\alpha$ to make them probability Borel measures $\nu_\alpha:=\iota_*\mu_\alpha$ on $\beta S$.
We want to take a convergent subnet of $\nu_\alpha\in\Prob(\beta S)$, and to show the limit is in fact contained in $\Prob(S)$.

Our first claim is that the measure $\nu_\alpha$ is regular for each $\alpha$, that is, $\nu_\alpha\in\Prob(\beta S)$.
For any Borel $E\subset\beta S$ and any $\e>0$, there is $F\subset E\cap S$ that is closed in $S$ such that $\mu_\alpha(E\cap S)<\mu_\alpha(F)+\e/2$ by inner regularity, and there is $K$ that is compact in $S$ such that $\mu_\alpha(S\setminus K)<\e/2$ by tightness.
Then, the inequality
\[\nu_\alpha(E)=\mu_\alpha(E\cap S)<\mu_\alpha(F)+\frac\e2<\mu_\alpha(F\cap K)+\e=\nu_\alpha(F\cap K)+\e\]
proves the regularity of $\nu_\alpha$ since $F\cap K$ is compact in both $S$ and $\beta S$ with $F\cap K\subset E$.
The space $\Prob(\beta S)$ is compact by the Banach-Alaoglu theorem and the Riesz-Markov-Kakutani representation theorem.
Therefore, $\nu_\alpha$ has a subnet $\nu_\beta$ that converges to $\nu\in\Prob(\beta S)$.

Recall that $\mu_\beta$ is tight.
For each $\e>0$, there is a compact $K\subset S$ such that $\nu_\beta(K)=\mu_\beta(K)\ge1-\e$ for all $\beta$.
Then, by the Portmanteau theorem, we have
\[\nu(S)\ge\nu(K)\ge\limsup_\beta\nu_\beta(K)\ge1-\e.\]
Since $\e$ is arbitrary, $\nu$ is concentrated on $S$, i.e. $\nu(S)=1$.
Now we restrict $\nu$ to $S$ in order to obtain $\mu$, which is a probability Borel measure on $S$.

From the definition of weak convergence we have
\[\int_{\beta S}f\,d\nu_\beta\to\int_{\beta S}f\,d\nu\]
for all $f\in C(\beta S)$.
Since $\nu_\beta(\beta S\setminus S)=\nu(\beta S\setminus S)=0$ and the restriction $C(\beta S)\to C_b(S)$ is an isomorphism due to the universal property of $\beta S$,
\[\int_Sf\,d\mu_\beta\to\int_Sf\,d\mu\]
for all $f\in C_b(S)$, so $\mu_\beta$ converges weakly to $\mu\in\Prob(S)$.
\end{pf}







\section{Characteristic functions}

\begin{prb}[Characteristic functions]
Let $\mu$ be a probability measure on $\R$.
Then, the \emph{characteristic function} of $\mu$ is defined by
\[\f(t):=Ee^{itX}=\int e^{itx}\,d\mu(x).\]
Note that $\f(t)=\hat\mu(-t)$ where $\hat\mu$ is the Fourier transform of $\mu\in\cS'(\R)$.
\begin{parts}
\item $\f\in C_b(\R)$.
\end{parts}
\end{prb}

\begin{prb}[Inversion formula]
Let $\mu$ be a probability measure on $\R$ and $\f$ its characteristic function.
\begin{parts}
\item For $a<b$, we have
\[\mu((a,b))+\frac12\mu(\{a,b\})=\lim_{T\to\infty}\frac1{2\pi}\int_{-T}^T\frac{e^{-ita}-e^{-itb}}{it}\f(t)\,dt.\]
\item For $a\in\R$, we have
\[\mu(\{a\})=\lim_{T\to\infty}\frac1{2T}\int_{-T}^Te^{-ita}\f(t)\,dt\]
\item If $\f\in L^1(\R)$, then $\mu$ has density
\[f(x)=\frac1{2\pi}\int e^{-itx}\f(t)\,dt\]
in $C_0(\R)\cap L^1(\R)$.
\end{parts}
\end{prb}

\begin{prb}[L\'evy's continuity theorem]
The continuity theorem provides with a tool to verify the weak convergence in terms of characteristic functions.
Let $\mu_n$ and $\mu$ be probability distributions on $\R$ with characteristic functions $\f_n$ and $\f$.
\begin{parts}
\item If $\mu_n\to\mu$ weakly, then $\f_n\to\f$ pointwise.
\item If $\f_n\to\f$ pointwise and $\f$ is continuous at zero, then $(\mu_n)_n$ is tight and $\mu_n\to\mu$ weakly.
\end{parts}
\end{prb}
\begin{pf}
(a)
For each $t$,
\[\f_n(t)=\int e^{itx}\,d\mu_n(x)\to\int e^{itx}\,d\mu(x)=\f(t)\]
because $e^{itx}\in C_b(\R)$.

(b)


\end{pf}

\begin{prb}[Criteria for characteristic functions]
Bochner's theorem and Polya's criterion
\end{prb}


There are two ways to represent a measure:
A measure $\mu$ is absolutely continuous iff its distribution $F$ is absolutely continuous iff its density $f$ is integrable.
So, the fourier transform of an absolutely continuous measure is just the fourier transform of $L^1$ functions.



\section{Moments}

moment problem

moment generating function defined on $|t|<\delta$


\section*{Exercises}
\begin{prb}
Let $\f_n$ be characteristic functions of probability measures $\mu_n$ on $\R$.
If there is a continuous function $\f$ such that $\f_n=\f$ on $n^{-1}\Z$, then $\mu_n$ converges weakly.
\end{prb}


\begin{prb}[Convergence determining class]
\end{prb}
%  generalization of Portemanteau theorem
%  uniformly continuous functions: They can separate two closed sets like Urysohn.


\begin{prb}[Vauge convergence]
Let $S$ be a locally compact Hausdorff space.
\begin{parts}
\item $\mu_\alpha\to\mu$ vaguely if and only if $\int g\,d\mu_\alpha\to\int g\,d\mu$ for all $g\in C_c(S)$.
\item $\mu_\alpha\to\mu$ weakly if and only if vaguely.
\item $\delta_n\to0$ vaguely but not weakly. (escaping to infinity)
\end{parts}
\end{prb}
\begin{pf}
\end{pf}
% C_0가 convergence determining class라는 말












\part{Discrete stochastic process}


\chapter{Limit theorems}

\section{Laws of large numbers}


Our purpose is to find appropriate $a_n$ and slowly growing $b_n$ such that $(S_n-a_n)/b_n\to0$ in probability or almost surely.


\begin{prb}[Kolmogorov-Feller theorem]
Let $X_i$ be an uncorrelated sequence of random variables such that
\[\lim_{x\to\infty}\sup_ixP(|X_i|>x)=0.\]
This condition is called the \emph{Kolmogorov-Feller} condition.
Let $Y_{n,i}:=X_i\1_{|X_i|\le c_n}$.
\begin{parts}
\item We have
\[\lim_{n\to\infty}P(S_n\ne T_n)=0\]
if $n\lesssim c_n$.
\item We have
\[\lim_{n\to\infty}P\left(\left|\frac{T_n-ET_n}{b_n}\right|>\e\right)=0\]
if $nc_n\lesssim b_n^2$.
\item We have
\[\frac{S_n-ET_n}n\to0\]
in probability.
\end{parts}
\end{prb}
\begin{pf}
Write $g(x):=\sup_ixP(|X_i|>x)$ so that $g(x)\to0$ as $x\to\infty$.

(a)
It follows from
\[P(S_n\ne T_n)\le\sum_{i=1}^nP(|X_i|>c_n)\le\sum_{i=1}^n\frac1{c_n}g(c_n)\lesssim g(c_n).\]

(b)
We write
\begin{align*}
P\left(\left|\frac{T_n-ET_n}{b_n}\right|>\e\right)
&\le\frac1{\e^2b_n^2}E|T_n-ET_n|^2\\
&=\frac1{\e^2b_n^2}\sum_{i=1}^nE|Y_{n,i}-EY_{n,i}|^2\\
&\le\frac1{\e^2b_n^2}\sum_{i=1}^nE|X_i\1_{|X_i|\le c_n}|^2\\
&=\frac1{\e^2b_n^2}\sum_{i=1}^n\int_0^{c_n}2xP(|X_i|>x)\,dx\\
&\le\frac{2n}{\e^2b_n^2}\int_0^{c_n}g(x)\,dx\\
&=\frac{2nc_n}{\e^2b_n^2}\int_0^1g(c_nx)\,dx\\
&\lesssim\int_0^1g(c_nx)\,dx.
\end{align*}
Since $g(x)\le x$ and $g(x)\to0$ as $x\to\infty$, the function $g$ is bounded.
By the bounded convergence theorem, we get $\int_0^1g(c_nx)\,dx\to0$ as $n\to\infty$.
\end{pf}


\begin{prb}[St. Petersburg paradox]
We want see the asymptotic behavior of the partial sums $S_n$ of i.i.d. random variables $X_i$ such that $E|X_i|=\infty$.
Let
\[P(X_n=2^m)=2^{-m}\quad\text{ for }m\ge1.\]
Let $Y_{n,i}:=X_i\1_{|X_i|\le c_n}$.
\begin{parts}
\item We have
\[\lim_{n\to\infty}P(S_n\ne T_n)=0\]
if $n\ll c_n$.
\item We have
\[\lim_{n\to\infty}P\left(\left|\frac{T_n-ET_n}{b_n}\right|>\e\right)=0\]
if $nc_n\ll b_n^2$.
\item We have
\[\frac{S_n-n\log_2n}{n^{1+\e}}\to0\]
in probability for every $\e>0$.
\end{parts}
\end{prb}
\begin{pf}
(a)
It follows from
\[P(S_n\ne T_n)
\le\sum_{i=1}^nP(X_i\ne Y_{n,i})
=\sum_{i=1}^nP(|X_i|>c_n)
\le\sum_{i=1}^n\frac2{c_n}=\frac{2n}{c_n}.\]

(b)
It follows from
\begin{align*}
P\left(\left|\frac{T_n-ET_n}{b_n}\right|>\e\right)
&\le\frac1{\e^2b_n^2}E|T_n-ET_n|^2\\
&=\frac1{\e^2b_n^2}\sum_{i=1}^nE|Y_{n,i}-EY_{n,i}|^2\\
&\le\frac1{\e^2b_n^2}\sum_{i=1}^nE|X_i\1_{|X_i|\le c_n}|^2\\
&\le\frac1{\e^2b_n^2}n\cdot2c_n
\end{align*}

\end{pf}



\begin{prb}[Borel-Cantelli lemmas]
\end{prb}

\begin{prb}[Head runs]
\end{prb}

\begin{prb}[Strong laws of large numbers for $L^1$]
Proof by Etemadi
\end{prb}

Random series proof

\section{Renewal theory}





\section{Central limit theorems}

\begin{prb}[Central limit theorem for $L^3$]
Replacement method by Lindeman and Lyapunov
\end{prb}

\begin{prb}[Lindeberg-Feller theorem]
Let $X_i$ be independent random variables such that for every $\e>0$ we have
\[\lim_{n\to\infty}\frac1{s_n^2}\sum_{i=1}^nE|X_i-EX_i|^2\1_{|X_i-EX_i|>\e s_n}=0.\]
This condition is called the \emph{Lindeberg-Feller} condition.
Let $Y_{n,i}:=\frac{X_i-EX_i}{s_n}$.
\begin{parts}
\item We have
\[|Ee^{it(S_n-ES_n)/s_n}-e^{-\frac12t^2}|\le\sum_{i=1}^n|Ee^{itY_{n,i}}-e^{-\frac12E(tY_{n,i})^2}|.\]
\item For any $\e>0$, we have an estimate
\[\Bigl|Ee^{itY}-\bigl(1-\frac12E(tY)^2\bigr)\Bigr|\lesssim_t\e EY^2+EY^2\1_{|Y|>\e}\]
for all random variables $Y$ such that $EY^2<\infty$.
\item For any $\e>0$, we have an estimate
\[\Bigl|e^{-\frac12E(tY)^2}-\bigl(1-\frac12E(tY)^2\bigr)\Bigr|
\lesssim_tEY^2(\e^2+EY^2\1_{|Y|>\e}).\]
for all random variables $Y$ such that $EY^2<\infty$.
\item
\end{parts}
\end{prb}
\begin{pf}
(a)
Note
\[Ee^{it(S_n-ES_n)/s_n}=\prod_{i=1}^nEe^{itY_{n,i}}\quad\text{ and }\quad e^{-\frac12t^2}=\prod_{i=1}^ne^{-\frac12E(tY_{n,i})^2}.\]

(b)
Since
\[\Bigl|e^{ix}-\bigl(1+ix-\frac12x^2\bigr)\Bigr|=\Bigl|\frac{i^3}2\int_0^x(x-y)^2e^{iy}\,dy\Bigr|\le\min\{\frac16|x|^3,x^2\}\]
for $x\in\R$, we have
\begin{align*}
\Bigl|Ee^{itY}-\bigl(1-\frac12E(tY)^2\bigr)\Bigr|
&\le E\Bigl|e^{itY}-\bigl(1-\frac12(tY)^2\bigr)\Bigr|\\
&\lesssim_t E\min\{|Y|^3,Y^2\}\\
&\le E|Y|^3\1_{|Y|\le\e}+EY^2\1_{|Y|>\e}\\
&\le\e EY^2+EY^2\1_{|Y|>\e}.
\end{align*}

(c)
Since
\[|e^{-x}-(1-x)|=\Bigl|\int_0^x(x-y)e^{-y}\,dy\Bigr|\le\frac12x^2\]
for $x\ge0$, we have
\[\Bigl|e^{-\frac12E(tY)^2}-\bigl(1-\frac12E(tY)^2\bigr)\Bigr|
\lesssim_t(EY^2)^2\le EY^2(\e^2+EY^2\1_{|Y|>\e}).\]
\end{pf}

\begin{prb}
Let $X_n:\Omega\to\R$ be independent random variables.
If there is $\delta>0$ such that the \emph{Lyapunov condition}
\[\lim_{n\to\infty}\frac1{s_n^{2+\delta}}\sum_{i=1}^nE|X_i-EX_i|^{2+\delta}=0\]
is satisfied, then
\[\frac{S_n-ES_n}{s_n}\to N(0,1)\]
weakly, where $S_n:=\sum_{i=1}^nX_i$ and $s_n^2:=VS_n$.
\end{prb}

% 여기에다가 truncation 까지 얹을 수 있음
% truncation level을 잡는 게 어려운 문제가 됨

Berry-Esseen ineaulity






\section*{Exercises}
\begin{prb}[Bernstein polynomial]
Let $X_n\sim\Bern(x)$ be i.i.d. random variables.
Since $S_n\sim\Binom(n,x)$, $E(S_n/n)=x$, $V(S_n/n)=x(1-x)/n$.
The $L^2$ law of large numbers implies $E(|S_n/n-x|^2)\to0$.
Define $f_n(x):=E(f(S_n/n))$.
Then, by the uniform continuity $|x-y|<\delta$ implies $|f(x)-f(y)|<\e$,
\[|f_n(x)-f(x)|\le E(|f(S_n/n)-f(x)|)\le\e+2\|f\|P(|S_n/n-x|\ge\delta)\to\e.\]
\end{prb}
\begin{prb}[High-dimensional cube is almost a sphere]
Let $X_n\sim\Unif(-1,1)$ be i.i.d. random variables and $Y_n:=X_n^2$.
Then, $E(Y_n)=\frac13$ and $V(Y_n)\le1$.
\end{prb}
\begin{prb}[Coupon collector's problem]
$T_n:=\inf\{\,t:|\{X_i\}_i|=n\,\}$
Since $X_{n,k}\sim\Geo(1-\frac{k-1}n)$, $E(X_{n,k})=(1-\frac{k-1}n)^{-1}$, $V(X_{n,k})\le(1-\frac{k-1}n)^{-2}$.
$E(T_n)\sim n\log n$
\end{prb}
\begin{prb}[An occupancy problem]
\end{prb}

\begin{prb}
Find the probability that arbitrarily chosen positive integers are coprime.
\end{prb}
Poisson convergence, law of rare events, or weak law of small numbers (a single sample makes a significant attibution)









\chapter{Martingales}
\section{Submartingales}
\section{Martingale convergence theorem}
\begin{prb}[Doob's upcrossing inequality]
\begin{parts}
\item
\end{parts}
\end{prb}

\begin{prb}[Martingale convergence theorems]
\begin{parts}
\item
\end{parts}
\end{prb}

\begin{prb}
\begin{parts}
\item
\end{parts}
\end{prb}

\section{Convergence in $L^p$ and uniform integrability}

\section{Optional stopping theorem}



\chapter{Markov chains}







\part{Continuous stochastic processes}





\chapter{Brownian motion}
\section{Kolomogorov extension}

\begin{prb}[Kolmogorov extension theorem]
A \emph{rectangle} is a finite product $\prod_{i=1}^nA_i\subset\R^n$ of measurable $A_i\subset\R$, and \emph{cylinder} is a product $A^*\times\R^\N$ where $A^*$ is a rectangle.
Let $\cA$ be the semi-algebra containing $\varnothing$ and all cylinders in $\R^\N$.
Let $(\mu_n)_n$ be a sequence of probability measures on $\R^n$ that satisfies \emph{consistency condition}
\[\mu_{n+1}(A^*\times\R)=\mu_n(A^*)\]
for any rectangles $A^*\subset\R^n$, and define a set function $\mu_0:\cA\to[0,\infty]$ by $\mu_0(A)=\mu_n(A^*)$ and $\mu_0(\varnothing)=0$.
\begin{parts}
\item $\mu_0$ is well-defined.
\item $\mu_0$ is finitely additive.
\item $\mu_0$ is countably additive if $\mu_0(B_n)\to0$ for cylinders $B_n\downarrow\varnothing$ as $n\to\infty$.
\item If $\mu_0(B_n)\ge\delta$, then we can find decreasing $D_n\subset B_n$ such that $\mu_0(D_n)\ge\frac\delta2$ and $D_n=D_n^*\times\R^\N$ for a compact rectangle $D_n^*$.
\item If $\mu_0(B_n)\ge\delta$, then $\bigcap_{i=1}^\infty B_i$ is non-empty.
\end{parts}
\end{prb}
\begin{pf}
(d)
Let $B_n=B_n^*\times\R^\N$ for a rectangle $B_n^*\subset\R^{r(n)}$.
By the inner regularity of $\mu_{r(n)}$, there is a compact rectangle $C_n^*\subset B_n^*$ such that
\[\mu_0(B_n\setminus C_n)=\mu_{r(n)}(B_n^*\setminus C_n^*)<\frac\delta{2^{n+1}}.\]
Let $C_n:=C_n^*\times\R^\N$ and define $D_n:=\bigcap_{i=1}^nC_i=D_n^*\times\R^\N$.
Then,
\[\mu_0(B_n\setminus D_n)\le\mu_0(\bigcup_{i=1}^nB_n\setminus C_i)\le\mu_0(\bigcup_{i=1}^nB_i\setminus C_i)<\frac\delta2,\]
which implies $\mu_0(D_n)\ge\frac\delta2$.

(e)
Take any sequence $(\omega_n)_n$ in $\R^\N$ such that $\omega_n\in D_n$.
Since each $D_n^*\subset\R^{r(n)}$ is compact and non-empty, by diagonal argument, we have a subsequence $(\omega_k)_k$ such that $\omega_k$ is pointwise convergent, and its limit is contained in $\bigcap_{i=1}^\infty D_i\subset\bigcap_{i=1}^\infty B_n=\varnothing$, which is a contradiction that leads $\mu_0(B_n)\to0$.
\end{pf}




\part{Stochastic calculus}

\end{document}
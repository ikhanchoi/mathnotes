\documentclass{../../large}
\usepackage{../../ikhanchoi}


\newcommand{\Unif}{\mathrm{Unif}}
\newcommand{\Bern}{\mathrm{Bern}}
\newcommand{\Binom}{\mathrm{Binom}}
\newcommand{\Geo}{\mathrm{Geo}}
\newcommand{\Exp}{\mathrm{Exp}}
\newcommand{\Pois}{\mathrm{Pois}}
\newcommand{\cadlag}{c\`adl\`ag }



\begin{document}
\title{Probability Theory}
\author{Ikhan Choi}
\maketitle
\tableofcontents


\part{Probability distributions}


\chapter{Random variables}

\section{Probability distributions}

\begin{prb}[Sample space]
A \emph{sample space} is a probability space, that is, a measure space $(\Omega,\cF,P)$ with $P(\Omega)=1$.
Elements and measurable subsets of a sample space are called \emph{outcomes} and \emph{events}, respectively.
Let $\Omega$ be a fixed sample space.
Then, a \emph{random element} is a measurable function $X:\Omega\to S$ to a measurable space $S$, called the \emph{state space}.
The state space $S$ is usally taken to be a Polish space together with its Borel $\sigma$-algebra.
If $S=\R$ or $\R^d$, then we call the random element $X$ as a \emph{random variable} or $\emph{random vector}$ respectively.

Consider a statistical study of ages of people in the earth at a time.
We conduct an experiment in which $n$ people are randomly chosen with replacement in order to verify a hypothesis.
We set the \emph{population} $\cP$ be the set of all people in the earth and the age function $a:\cP\to\Z_{\ge0}$.
If we denote by $X_i$ the age of $i$th person, then the reasonable choice for the domain of the random variables $X_i$ is $\Omega=\cP^n$, since the independence of $X_i$ and $X_j$ for $i\ne j$ can be easily realized by defining $X_i(p_1,p_2,\cdots):=a(p_i)$ by the product measure.
In probability theory and statistics, we are interested in the distribution of age, that is, the estimation of the size of $a^{-1}(k)$ for each $k\in\Z_{\ge0}$, not in the exact description of the age function $a$, and it is expected to be achieved approximately as $n$ tends to infinity.
Believing the determinism, an experiment is in fact recognized as an operation of revealing a pre-determined fate $\omega$ in the universal space $\Omega$ of possible world lines.
The sample space $\Omega$ can be sufficiently enlarged when we require a finer domain of discourse such as the case $n\to\infty$, and we do not care of any concrete description of $\Omega$ except when discussing the mathematical existence issues.
\end{prb}

\begin{prb}[Probability distribution]
Let $X:\Omega\to S$ be a random element, where $S$ is a topological space.
The (probability) \emph{distribution} of $X$ is the pushforward measure $X_*P$ on $\R$.
The right continuous non-decreasing function $F$ corresponded to $X_*P$ is called the (cumulative) \emph{distribution function}.

If the distribution has discrete support, then we say $X$ is \emph{discrete}.
Since a probability measure of discrete support is a countable convex combination of Dirac measures, we can define the (probability) \emph{mass function} $p:\supp(X_*P)\to[0,1]$.
If the distribution is absolutely continuous with respect to the Lebesgue measure, then we say $X$ is \emph{continuous}.
By the Radon-Nikodym theorem, we can define the (probability) \emph{density function} $f\in L^1(\R)$.
The mass and density functions are effective ways to describe distributions of random variables in most applications.
\begin{parts}
\item Every single probability Borel measure on $S$ is regular if $S$ is perfectly normal. (inner approximation by closed sets)
\item Every single probability Borel measure is tight if $S$ is Polish. (inner approximation by compact sets)
\end{parts}
\end{prb}


\begin{prb}[Expectation and moments]
Chebyshev's inequality
\end{prb}
\begin{prb}[Joint distribution]
\end{prb}
\begin{prb}[Distribution of functions]
transformation, function
\end{prb}


\section{Discrete distributions}

\section{Continuous distributions}



\section*{Exercises}

equally likely outcomes
	coin toss
	dice roll
	ball drawing
	number permutation
	life time of a light bulb






\chapter{Independence}


\begin{prb}[Dynkin's $\pi$-$\lambda$ lemma]
Let $\cP$ be a $\pi$-system and $\cL$ a $\lambda$-system respectively.
Denote by $\ell(\cP)$ the smallest $\lambda$-system containing $\cP$.
\begin{parts}
\item If $A\in\ell(\cP)$, then $\cG_A:=\{B:A\cap B\in\ell(\cP)\}$ is a $\lambda$-system.
\item $\ell(\cP)$ is a $\pi$-system.
\item If a $\lambda$-system is a $\pi$-system, then it is a $\sigma$-algebra.
\item If $\cP\subset\cL$, then $\sigma(\cP)\subset\cL$.
\end{parts}
\end{prb}

\begin{prb}[Monotone class lemma]

\end{prb}

\begin{prb}[Kolmogorov extension theorem]
Let $\{S_i\}_{i\in I}$ be a family of Polish spaces and 
consider the product $S=\prod_{i\in I}S_i$ with projections $\pi_i:S\to S_i$ and $\pi_J:S\to\prod_{j\in J}S_j$ for finite $J\subset I$.
A \emph{cylinder set} is a set of the form $\pi_J^{-1}(A)\subset S$ for a measurable $A\in S_J$.
Let $\cA$ be the semi-algebra containing $\varnothing$ and all cylinders in $S_I$.
Let $(\mu_J)_J$ be a net of probability measures on $S_I$ satisfying $\sigma(\mu_J)\subset\sigma(\pi_J)$ and the \emph{consistency condition}.
Define a set function $\mu_0:\cA\to[0,\infty]$ by $\mu_0(A)=\mu_n(A^*)$ and $\mu_0(\varnothing)=0$.
\begin{parts}
\item $\mu_0$ is well-defined.
\item $\mu_0$ is finitely additive.
\item $\mu_0$ is countably additive if $\mu_0(B_n)\to0$ for cylinders $B_n\downarrow\varnothing$ as $n\to\infty$.
\item If $\mu_0(B_n)\ge\delta$, then we can find decreasing $D_n\subset B_n$ such that $\mu_0(D_n)\ge\frac\delta2$ and $D_n=D_n^*\times\R^\N$ for a compact rectangle $D_n^*$.

\end{parts}
\end{prb}
\begin{pf}


(d)
Let $B_n=B_n^*\times\R^\N$ for a rectangle $B_n^*\subset\R^{r(n)}$.
By the inner regularity of $\mu_{r(n)}$, there is a compact rectangle $C_n^*\subset B_n^*$ such that
\[\mu_0(B_n\setminus C_n)=\mu_{r(n)}(B_n^*\setminus C_n^*)<\frac\delta{2^{n+1}}.\]
Let $C_n:=C_n^*\times\R^\N$ and define $D_n:=\bigcap_{i=1}^nC_i=D_n^*\times\R^\N$.
Then,
\[\mu_0(B_n\setminus D_n)\le\mu_0(\bigcup_{i=1}^nB_n\setminus C_i)\le\mu_0(\bigcup_{i=1}^nB_i\setminus C_i)<\frac\delta2,\]
which implies $\mu_0(D_n)\ge\frac\delta2$.

Take any sequence $(\omega_n)_n$ in $\R^\N$ such that $\omega_n\in D_n$.
Since each $D_n^*\subset\R^{r(n)}$ is compact and non-empty, by diagonal argument, we have a subsequence $(\omega_k)_k$ such that $\omega_k$ is pointwise convergent, and its limit is contained in $\bigcap_{i=1}^\infty D_i\subset\bigcap_{i=1}^\infty B_n=\varnothing$, which is a contradiction that leads $\mu_0(B_n)\to0$.
\end{pf}


\section{Conditional probability}
filtered probability space
disintegration







\section*{Exercises}

\begin{prb}[Monty Hall problem]
Suppose you are on a game show, and given the choice of three doors $A$, $B$, and $C$.
Behind one door is a car; behind the others, goats.
You know that the probabilities $a$, $b$, and $c=1-a-b$.
You pick a door, say $A$, and the host, who knows what's behind the doors, opens another door, say $B$, which has a goat.
He then says to you, ``Do you want to pick door $C$?''
Is it to your advantage to switch your choice?
\begin{parts}
\item Find the condition for $a,b,c$ that the participant benefits when changed the choice.
\end{parts}
\end{prb}
\begin{pf}
Let $A$, $B$, and $C$ be the events that a car is behind the doors $A$, $B$, and $C$, respectively.
Let $X$ the event that the game host opened $B$.
Note $\{A,B,C\}$ is a partition of the sample space $\Omega$, and $X$ is independent to $A$, $B$, and $C$.
Then, $P(A)=P(B)=P(C)=1/3$, and
\[P(X|A)=\frac12,\quad P(X|B)=0,\quad P(X|C)=1.\]
Therefore,
\begin{align*}
P(C|X)=\frac{P(X\cap C)}{P(X)}
&=\frac{P(X|C)P(C)}{P(X|A)P(A)+P(X|B)P(B)+P(X|C)P(C)}\\
&=\frac{1\cdot \frac13}{\frac12\cdot\frac13+0\cdot\frac13+1\cdot\frac13}=\frac23.
\end{align*}
Similarly, $P(A|X)=\frac13$ and $P(B|X)=0$.
\end{pf}










\chapter{Convergence of distributions}











\section{Convergence in distribution}




% pi system that generates sigma alg is separating
% in \R^\infty and C(I), set of finite dimensional sets is separating




\begin{prb}[Portmanteau theorem]
Let $S$ be a normal space.
We say a net $\mu_\alpha$ in $\Prob(S)$ \emph{converges in distribution} or \emph{weakly} to $\mu$ if
\[\int f\,d\mu_\alpha\to\int f\,d\mu,\qquad f\in C_b(S).\]
The following statements are all equivalent.
\begin{parts}
\item $\mu_\alpha\to\mu$ in distribution.
\item $\mu_\alpha(g)\to\mu(g)$ for every uniformly continuous $g\in C_b(S)$.
\item $\limsup_\alpha\mu_\alpha(F)\le\mu(F)$ for every closed $F\subset S$.
\item $\liminf_\alpha\mu_\alpha(U)\ge\mu(U)$ for every open $U\subset S$.
\item $\lim_\alpha\mu_\alpha(A)=\mu(A)$ for every Borel $A\subset S$ such that $\mu(\partial A)=0$.
\end{parts}
\end{prb}
\begin{pf}
(a)$\Rightarrow$(b)
Clear.

(b)$\Rightarrow$(c)
Let $U$ be an open set such that $F\subset U$.
There is uniformly continuous $g\in C_b(S)$ such that $\1_F\le g\le\1_U$.
Therefore,
\[\limsup_\alpha\mu_\alpha(F)\le\limsup_\alpha\mu_\alpha(g)=\mu(g)\le\mu(U).\]
By the outer regularity of $\mu$, we obtain $\limsup_\alpha\mu_\alpha(F)\le\mu(F)$.

(c)$\Leftrightarrow$(d)
Clear.

(c)$+$(d)$\Rightarrow$(e)
It easily follows from
\[\limsup_\alpha\mu_\alpha(\bar A)\le\mu(\bar A)=\mu(A)=\mu(A^\circ)\le\liminf_\alpha\mu_\alpha(A^\circ).\]

(e)$\Rightarrow$(a)
Let $g\in C_b(S)$ and $\e>0$.
Since the pushforward measrue $g_*\mu$ has at most countably many mass points, there is a partition $(t_i)_{i=0}^n$ of an interval containing $[-\|g\|,\|g\|]$ such that $|t_{i+1}-t_i|<\e$ and $\mu(\{x:g(x)=t_i\})=0$ for each $i$.
Let $(A_i)_{i=0}^{n-1}$ be a Borel decomposition of $S$ given by $A_i:=g^{-1}([t_i,t_{i+1}))$, and define $f_\e:=\sum_{i=0}^{n-1}t_i\1_{A_i}$ so that we have $\sup_{x\in S}|g_\e(x)-g(x)|\le\e$.
From
\begin{align*}
|\mu_\alpha(g)-\mu(g)|&\le|\mu_\alpha(g-g_\e)|+|\mu_\alpha(g_\e)-\mu(g_\e)|+|\mu(g_\e-g)|\\
&\le\e+\sum_{i=0}^{n-1}|t_i||\mu_\alpha(A_i)-\mu(A_i)|+\e,
\end{align*}
we get
\[\limsup_\alpha|\mu_\alpha(g)-\mu(g)|<2\e.\]
Since $\e$ is arbitrary, we are done.
\end{pf}



\begin{prb}[L\'evy-Prokhorov metric]
Let $S$ be a metric space, and $\Prob(S)$ be the set of probability (regular) Borel measures on $S$.
Define $\pi:\Prob(S)\times\Prob(S)\to[0,\infty)$ such that
\[\pi(\mu,\nu):=\inf\{\,r>0:\mu(A)\le\nu(B(A,r))+r,\ \nu(A)\le\mu(B(A,r))+r,\ \forall A\in\cB(S)\,\},\]
where $B(A,r):=\bigcup_{a\in A}B(a,r)$.
\begin{parts}
\item $\pi$ is a metric.
\item If $\mu_n\to\mu$ in $\pi$, then $\mu_n\to\mu$ in distribution.
\item If $\mu_\alpha\to\mu$ in distribution, then $\mu_\alpha\to\mu$ in $\pi$, if $S$ is separable.
\item $(S,d)$ is separable if and only if $(\Prob(S),\pi)$ is separable.
\item $(S,d)$ is compact if and only if $(\Prob(S),\pi)$ is compact
\item $(S,d)$ is complete if and only if $(\Prob(S),\pi)$ is complete.
\end{parts}
\end{prb}
\begin{pf}
(c)
\end{pf}



\begin{prb}[Prokhorov theorem]
Let $S$ be a Polish space.
Let $\Prob(S)$ be the space of probability measures on $S$ endowed with the topology of convergence in distribution.
Let $M\subset\Prob(S)$.
We say $M$ is \emph{tight} if for each $\e>0$ there is compact $K\subset S$ such that
\[\inf_{\mu\in M}\mu(K)>1-\e.\]
\begin{parts}
\item If $M$ is relatively compact, then it is tight.
\item If $M$ is tight, then it is relatively compact.
\end{parts}
\end{prb}
\begin{pf}
(a)
Fix $\e>0$.
We first claim as a lemma that for an open cover $\{B_i\}_{i\in I}$ of $S$ we have
\[\sup_J\inf_{\mu\in M}\mu(B_J)=1,\]
where $B_J:=\bigcup_{j\in J}B_j$ and $J$ runs through all finite subsets of $I$.
Suppose the claim is false so that there are $\e>0$ and a net $(\mu_J)$ in $M$ such that $\mu_J(B_J)\le1-\e$.
Because $\bar M$ is compact, we have a subnet $\mu_{J_\alpha}$ of $\mu_J$ that converges to $\mu\in\bar M$ in distribution, then by the Portmanteau theorem we have for any finite $J\subset I$ that
\[\mu(B_J)\le\liminf_\alpha\mu_{J_\alpha}(B_J)\le\liminf_\alpha\mu_\alpha(B_{J_\alpha})\le1-\e.\]
By limiting $J\uparrow I$, we lead a contradiction, so the claim is verified.

Now we use that $S$ is Polish.
Let $\{x_i\}_{i=1}^\infty$ be a dense set in $S$.
Fix a metric $d$ on $S$ and consider the family of open covers of balls $\{B(x_i,m^{-1})\}$ parametrized by integers $m$.
By the above claim, there is a finite $n_m>0$ such that
\[\inf_{\mu\in M}\mu\Bigl(\bigcup_{i=1}^{n_m}B(x_i,m^{-1})\Bigr)>1-\frac\e{2^m}.\]
Define
\[K:=\bigcap_{m=1}^\infty\bigcup_{i=1}^{n_m}\bar{B(x_i,m^{-1})},\]
which compact since $S$ is complete in $d$ and it is closed and totally bounded.
Moreover, we can verify
\[1-\mu(K)=\mu\Bigl(\bigcup_{m=1}^\infty\bigcap_{i=1}^{n_m}\bar{B(x_i,\tfrac1m)}^c\Bigr)\le\sum_{m=1}^\infty\left(1-\mu\Bigl(\bigcup_{i=1}^{n_m}B(x_i,\tfrac1m)\Bigr)\right)<\e\]
for every $\mu\in M$, so $M$ is tight.

(b)
We first prove that we have a natural embedding $i_*:\Prob(S)\to\Prob(\beta S)$ with respect to the topology of convergence in distribution, where $\beta S$ is the Stone-\v Cech compactification and the map $i_*$ is the pushforward of the natural embedding $i:S\to\beta S$ taken thanks to that $S$ is completely regular.
Be cautious that the space $\Prob(\beta S)$ is defined to be the space of probability regular Borel measures on $\beta S$ because $\beta S$ is no more metrizable.
Let $\mu\in\Prob(S)$ and $\nu:=i_*\mu$.
Since $\nu$ is cleary a probability Borel measure on $\beta S$, so we prove it is regular.
For any Borel $E\subset\beta S$ and any $\e>0$, there is relatively closed $F\subset E\cap S$ in $S$ such that $\mu(E\cap S)<\mu(F)+\e/2$ by the inner regularity of $\mu$, and there is $K$ that is compact in $S$ such that $\mu(S\setminus K)<\e/2$ by the tightness of $\mu$.
Then, the inequality
\[\nu(E)=\mu(E\cap S)<\mu(F)+\frac\e2<\mu(F\cap K)+\e=\nu(F\cap K)+\e\]
proves that $\nu$ is regular since $F\cap K$ is closed in $\beta S$ by compactness and satisfies $F\cap K\subset E$.
Now we prove that for a net $(\mu_\alpha)$ in $\Prob(S)$, if $\nu_\alpha:=i_*\mu_\alpha\to\nu:=i_*\mu$ in distribution, then $\mu_\alpha\to\mu$ in distribution.
By assumption, we have
\[\int_{\beta S}f\,d\nu_\alpha\to\int_{\beta S}f\,d\nu,\qquad f\in C(\beta S).\]
Since $\nu_\alpha(\beta S\setminus S)=\nu(\beta S\setminus S)=0$ and the restriction $C(\beta S)\to C_b(S)$ is an isomorphism due to the universal property of $\beta S$, we have
\[\int_Sf\,d\mu_\alpha\to\int_Sf\,d\mu,\qquad f\in C_b(S),\] so $\mu_\alpha\to\mu$ in distribution.
Hence, we have the embedding $i_*:\Prob(S)\to\Prob(\beta S)$.

Let $M$ be a tight subset of $\Prob(S)$.
Let $(\mu_\alpha)$ be a net in $M$.
Because the topology of convergence in distribution on $\Prob(\beta S)$ is compact by the Banach-Alaoglu theorem and the Riesz-Markov-Kakutani representation theorem, the net of regular Borel measures $\nu_\alpha:=i_*\mu_\alpha$ has a subnet $\nu_\beta$ that converges to $\nu\in\Prob(\beta S)$ in distribution.
By the tightness of $\{\mu_\beta\}$, for each $\e>0$, there is compact $K\subset S$ such that $\nu_\beta(K)=\mu_\beta(K)\ge1-\e$ for all $\beta$.
Then, by the Portmanteau theorem, we have
\[\nu(S)\ge\nu(K)\ge\limsup_\beta\nu_\beta(K)\ge1-\e.\]
Since $\e$ is arbitrary, $\nu$ is concentrated on $S$, i.e.~$\nu(S)=1$, which means that $\nu$ is contained the image of $\Prob(S)$.
By restriction $\nu$ on $S$ we obtain $\mu$, the limit of $\mu_\beta$.
\end{pf}


\begin{prb}[Skorokhod representation theorem]
\end{prb}


\begin{prb}[Continuous mapping theorem]
\end{prb}


\begin{prb}[Slutsky theorem]
\end{prb}


\section{Characteristic functions}

\begin{prb}[Characteristic functions]
Let $\mu$ be a probability Borel measure on $\R$.
Then, the \emph{characteristic function} of $\mu$ is a function $\f:\R\to\C$ defined by
\[\f(t):=Ee^{itX}=\int e^{itx}\,d\mu(x).\]
Note that $\f(t)=\hat\mu(-t)$ where $\hat\mu$ is the Fourier transform of $\mu\in\Prob(S)\subset\cS'(\R)$.
\begin{parts}
\item $\f\in C_b(\R)$.
\end{parts}
\end{prb}

\begin{prb}[Inversion formula]
Let $\mu$ be a probability Borel measure on $\R$ and $\f$ its characteristic function.
\begin{parts}
\item For $a<b$, we have
\[\mu((a,b))+\frac12\mu(\{a,b\})=\lim_{T\to\infty}\frac1{2\pi}\int_{-T}^T\frac{e^{-ita}-e^{-itb}}{it}\f(t)\,dt.\]
\item For $a\in\R$, we have
\[\mu(\{a\})=\lim_{T\to\infty}\frac1{2T}\int_{-T}^Te^{-ita}\f(t)\,dt\]
\item If $\f\in L^1(\R)$, then $\mu$ has density
\[f(x)=\frac1{2\pi}\int e^{-itx}\f(t)\,dt\]
in $C_0(\R)\cap L^1(\R)$.
\end{parts}
\end{prb}

\begin{prb}[L\'evy's continuity theorem]
The continuity theorem provides with a tool to verify the weak convergence in terms of characteristic functions.
Let $\mu_n$ and $\mu$ be probability distributions on $\R$ with characteristic functions $\f_n$ and $\f$.
\begin{parts}
\item If $\mu_n\to\mu$ in distribution, then $\f_n\to\f$ pointwise.
\item If $\f_n\to\f$ pointwise and $\f$ is continuous at zero, then $(\mu_n)$ is tight and $\mu_n\to\mu$ in distribution.
\end{parts}
\end{prb}
\begin{pf}
(a)
For each $t$,
\[\f_n(t)=\int e^{itx}\,d\mu_n(x)\to\int e^{itx}\,d\mu(x)=\f(t)\]
because $e^{itx}\in C_b(\R)$.

(b)


\end{pf}

\begin{prb}[Criteria for characteristic functions]
Bochner's theorem and Polya's criterion
\end{prb}


There are two ways to represent a measure:
A measure $\mu$ is absolutely continuous iff its distribution $F$ is absolutely continuous iff its density $f$ is integrable.
So, the fourier transform of an absolutely continuous measure is just the fourier transform of $L^1$ functions.



\section{Moments}

moment problem

moment generating function defined on $|t|<\delta$


\section*{Exercises}


\begin{prb}[Local limit theorems]
Suppose $f_n$ and $f$ are density functions.
\begin{parts}
\item If $f_n\to f$ a.e., then $f_n\to f$ in $L^1$.\hfill(Scheff\'e's theorem)
\item $f_n\to f$ in $L^1$ if and only if in total variation.
\item If $f_n\to f$ in total variation, then $f_n\to f$ in distribution.
\end{parts}
\end{prb}

\begin{prb}[Convergence on real line]\,
\begin{parts}
\item Portmanteau: 
$F_n(x)\to F(x)$ for every continuity point $x$ of $F$.
\item Easy proof of the Skorokhod representation
\item Easy proof of continuous mapping theorem
\item Easy proof of the Slutsky theorem
\item Helly selection theorem, which uses $S^1$ instead of $\beta\R$.
\end{parts}
\end{prb}

\begin{prb}[Embedding by Dirac measures]
Let $S$ be a normal space.
\begin{parts}
\item $S\to\Prob(S)$ is a topological embedding.
\item $S\subset\Prob(S)$ is sequentially closed.
\item % If a metric is given, then completeness?
\end{parts}
\end{prb}
\begin{pf}
(a)
It uses Urysohn.

(b)
It uses (b)=>(c) of Portmanteau.
\end{pf}

\begin{prb}
Let $\f_n$ be characteristic functions of probability measures $\mu_n$ on $\R$.
If there is a continuous function $\f$ such that $\f_n=\f$ on $n^{-1}\Z$, then $\mu_n$ converges weakly.
\end{prb}


\begin{prb}[Convergence determining class]
\end{prb}
%  generalization of Portemanteau theorem
%  uniformly continuous functions: They can separate two closed sets like Urysohn.


\begin{prb}[Vauge convergence]
Let $S$ be a locally compact Hausdorff space.
\begin{parts}
\item $\mu_\alpha\to\mu$ vaguely if and only if $\int g\,d\mu_\alpha\to\int g\,d\mu$ for all $g\in C_c(S)$.
\item $\mu_\alpha\to\mu$ weakly if and only if vaguely.
\item $\delta_n\to0$ vaguely but not weakly. (escaping to infinity)
\end{parts}
\end{prb}
\begin{pf}
\end{pf}
% C_0가 convergence determining class라는 말












\part{Stochastic processes}


\chapter{Limit theorems}

\section{Laws of large numbers}


\begin{prb}[Weak law of large numbers]
Let $(X_i)$ be an uncorrelated sequence of random variables, that is, $E(X_iX_j)=EX_iEX_j$ for all $i,j$.
Define
\[g(x):=\sup_ixP(|X_i|>x).\]
Note that for any $\e>0$, $\sup_iE|X_i|<\infty$ implies $\sup_xg(x)<\infty$, which implies $\sup_iE|X_i|^{1-\e}<\infty$.
In particular, the condition $\lim_{x\to\infty}g(x)=0$ is called the Kolmogorov-Feller condition.
Consider the truncation $Y_{n,i}:=X_i\1_{|X_i|\le c_n}$.
\begin{parts}
\item If $(n/c_n)g(c_n)\to0$, then
\[P(S_n\ne T_n)\to0.\]
\item If $(nc_n/b_n^2)\int_0^\infty g(c_nx)\,dx\to0$, then
\[P\left(\left|\frac{T_n-ET_n}{b_n}\right|>\e\right)\to0.\]
\item If the above two conditions are satisfied and $a_n\sim ET_n$, then
\[\frac{S_n-a_n}{b_n}\to0\qquad\text{in probability}.\]
\end{parts}
\end{prb}
\begin{pf}
(a)
Write $g(x):=\sup_ixP(|X_i|>x)$ so that $g(x)\to0$ as $x\to\infty$.
It follows from
\[P(S_n\ne T_n)\le\sum_{i=1}^nP(|X_i|>c_n)\le\sum_{i=1}^n\frac1{c_n}g(c_n)=\frac{ng(c_n)}{c_n}\to0.\]
If the Kolmogorov-Feller condition holds, then we may let $c_n\sim n$.

(b)
We write
\begin{align*}
P\left(\left|\frac{T_n-ET_n}{b_n}\right|>\e\right)
&\le\frac1{\e^2b_n^2}E|T_n-ET_n|^2\\
&=\frac1{\e^2b_n^2}\sum_{i=1}^nE|Y_{n,i}-EY_{n,i}|^2\\
&\le\frac1{\e^2b_n^2}\sum_{i=1}^nE|X_i\1_{|X_i|\le c_n}|^2\\
&=\frac1{\e^2b_n^2}\sum_{i=1}^n\int_0^{c_n}2xP(|X_i|>x)\,dx\\
&\le\frac{2n}{\e^2b_n^2}\int_0^{c_n}g(x)\,dx\\
&=\frac{2nc_n}{\e^2b_n^2}\int_0^1g(c_nx)\,dx.
\end{align*}
We are done.
If the Kolmogorov-Feller condition holds, then we may let $nc_n\sim b_n^2$ by the bounded convergence theorem.

(c)
From the part (a) and (b) we have
\[P\left(\left|\frac{S_n-ET_n}n\right|>\e\right)\le P(S_n\ne T_n)+P\left(\left|\frac{T_n-ET_n}{b_n}\right|>\e\right)\to0.\qedhere\]
\end{pf}


\begin{prb}[Borel-Cantelli lemmas]
\end{prb}


\begin{prb}[Kolmogorov maximal inequality]
If $(X_i)$ is the sequence of independent random variables such that $EX_i=0$ and $VX_i<\infty$, then
\[P(S^*_n>\e)\le\frac1{\e^2}VS_n,\]
where $S^*_n:=\max_{i\le n}|S_i|$.
We can prove it by construction of a linear martingale $S_{n\wedge\tau}$ with a stopping time to hit $\e$: independence and zero mean are necessary.
This is a special case of the Doob maximal inequality for $S_{n\wedge\tau}^2$.
\end{prb}


\begin{prb}[Kolmogorov three series theorem]
Let $(X_i)$ be a sequence of independent random variables.
Suppose for a constant $c>0$ and $Y_i:=X_i\1_{|X_i|\le c}$  that the following three series are convergent:
\[\sum_{i=1}^\infty P(|X_i|>c),\qquad\sum_{i=1}^\infty EY_i,\qquad\sum_{i=1}^\infty VY_i.\]
\end{prb}

\begin{prb}[Strong laws of large numbers]
Let $(X_i)$ be a sequence of independent random variables.
The Kolmogorov condition:
\[\sum_{n=1}^\infty\frac{E|Y_n|^2}{b_n^2}<\infty.\]
It is satisfied when $E|X_i|<\infty$.
Kronecker lemma
\end{prb}


\begin{prb}[Etemadi theorem]
Extend the theorem for pairwise independent.
But for pairwise uncorrelated, we need a lower bound.
By extracting a exponentially fast but sparse subsequence, prove the a.s.~convergence.
And as we do in renewel theory, we may assume the sequence is non-decreasing and apply the squeeze.
\end{prb}

\section{Renewal theory}





\section{Central limit theorems}

\begin{prb}[Central limit theorem for $L^3$]
Replacement method by Lindeman and Lyapunov
\end{prb}

\begin{prb}[Lindeberg-Feller theorem]
Let $X_i$ be independent random variables such that for every $\e>0$ we have
\[\lim_{n\to\infty}\frac1{s_n^2}\sum_{i=1}^nE|X_i-EX_i|^2\1_{|X_i-EX_i|>\e s_n}=0.\]
This condition is called the \emph{Lindeberg-Feller} condition.
Let $Y_{n,i}:=\frac{X_i-EX_i}{s_n}$.
\begin{parts}
\item We have
\[|Ee^{it(S_n-ES_n)/s_n}-e^{-\frac12t^2}|\le\sum_{i=1}^n|Ee^{itY_{n,i}}-e^{-\frac12E(tY_{n,i})^2}|.\]
\item For any $\e>0$, we have an estimate
\[\Bigl|Ee^{itY}-\bigl(1-\frac12E(tY)^2\bigr)\Bigr|\lesssim_t\e EY^2+EY^2\1_{|Y|>\e}\]
for all random variables $Y$ such that $EY^2<\infty$.
\item For any $\e>0$, we have an estimate
\[\Bigl|e^{-\frac12E(tY)^2}-\bigl(1-\frac12E(tY)^2\bigr)\Bigr|
\lesssim_tEY^2(\e^2+EY^2\1_{|Y|>\e}).\]
for all random variables $Y$ such that $EY^2<\infty$.
\item
\end{parts}
\end{prb}
\begin{pf}
(a)
Note
\[Ee^{it(S_n-ES_n)/s_n}=\prod_{i=1}^nEe^{itY_{n,i}}\quad\text{ and }\quad e^{-\frac12t^2}=\prod_{i=1}^ne^{-\frac12E(tY_{n,i})^2}.\]

(b)
Since
\[\Bigl|e^{ix}-\bigl(1+ix-\frac12x^2\bigr)\Bigr|=\Bigl|\frac{i^3}2\int_0^x(x-y)^2e^{iy}\,dy\Bigr|\le\min\{\frac16|x|^3,x^2\}\]
for $x\in\R$, we have
\begin{align*}
\Bigl|Ee^{itY}-\bigl(1-\frac12E(tY)^2\bigr)\Bigr|
&\le E\Bigl|e^{itY}-\bigl(1-\frac12(tY)^2\bigr)\Bigr|\\
&\lesssim_t E\min\{|Y|^3,Y^2\}\\
&\le E|Y|^3\1_{|Y|\le\e}+EY^2\1_{|Y|>\e}\\
&\le\e EY^2+EY^2\1_{|Y|>\e}.
\end{align*}

(c)
Since
\[|e^{-x}-(1-x)|=\Bigl|\int_0^x(x-y)e^{-y}\,dy\Bigr|\le\frac12x^2\]
for $x\ge0$, we have
\[\Bigl|e^{-\frac12E(tY)^2}-\bigl(1-\frac12E(tY)^2\bigr)\Bigr|
\lesssim_t(EY^2)^2\le EY^2(\e^2+EY^2\1_{|Y|>\e}).\]
\end{pf}

\begin{prb}
Let $X_n:\Omega\to\R$ be independent random variables.
If there is $\delta>0$ such that the \emph{Lyapunov condition}
\[\lim_{n\to\infty}\frac1{s_n^{2+\delta}}\sum_{i=1}^nE|X_i-EX_i|^{2+\delta}=0\]
is satisfied, then
\[\frac{S_n-ES_n}{s_n}\to N(0,1)\]
weakly, where $S_n:=\sum_{i=1}^nX_i$ and $s_n^2:=VS_n$.
\end{prb}

% 여기에다가 truncation 까지 얹을 수 있음
% truncation level을 잡는 게 어려운 문제가 됨

Berry-Esseen ineaulity






\section*{Exercises}
\begin{prb}[Bernstein polynomial]
Let $X_n\sim\Bern(x)$ be i.i.d. random variables.
Since $S_n\sim\Binom(n,x)$, $E(S_n/n)=x$, $V(S_n/n)=x(1-x)/n$.
The $L^2$ law of large numbers implies $E(|S_n/n-x|^2)\to0$.
Define $f_n(x):=E(f(S_n/n))$.
Then, by the uniform continuity $|x-y|<\delta$ implies $|f(x)-f(y)|<\e$,
\[|f_n(x)-f(x)|\le E(|f(S_n/n)-f(x)|)\le\e+2\|f\|P(|S_n/n-x|\ge\delta)\to\e.\]
\end{prb}
\begin{prb}[High-dimensional cube is almost a sphere]
Let $X_n\sim\Unif(-1,1)$ be i.i.d. random variables and $Y_n:=X_n^2$.
Then, $E(Y_n)=\frac13$ and $V(Y_n)\le1$.
\end{prb}
\begin{prb}[Coupon collector's problem]
$T_n:=\inf\{\,t:|\{X_i\}_i|=n\,\}$
Since $X_{n,k}\sim\Geo(1-\frac{k-1}n)$, $E(X_{n,k})=(1-\frac{k-1}n)^{-1}$, $V(X_{n,k})\le(1-\frac{k-1}n)^{-2}$.
$E(T_n)\sim n\log n$
\end{prb}
\begin{prb}[An occupancy problem]
\end{prb}
\begin{prb}[St.~Peterburg paradox]
For $P(X_n=2^m)=2^{-m}$, $g\le1$ so that $(S_n-n\log_2n)/n^{1+\e}\to0$ in probability.
\end{prb}

\begin{prb}[Head runs]
\end{prb}

\begin{prb}
Find the probability that arbitrarily chosen positive integers are coprime.
\end{prb}
Poisson convergence, law of rare events, or weak law of small numbers (a single sample makes a significant attibution)









\chapter{Discrete stochastic processes}

\section{Martingales}
In this chapter we do not use the countability of the index set $\N$.

\begin{prb}
\begin{parts}
\item If $EX_n=0$, then $S_n$ is a martingale.
\item If $EX_n=0$ and $VX_n=\sigma^2$, then $S_n^2-n\sigma^2$ is a martingale.
\item If $EX_n=1$ and $X_n\ge0$, then $M_n:=\prod_{i=1}^nX_i$ is a martingale.
\item If $X_n$ is a martingel and $\f$ is convex, then $\f(X_n)$ is a submartingale.
\item If $X_n$ is a submartingale and $\f$ is non-decreasing convex, then $\f(X_n)$ is a submartingale.
\item If $H_n\ge0$ is predictable and $X_n$ is a (super/sub)martingale, then the \emph{(super/sub)martingale transform}
\[(H\cdot X)_n:=H_1X_1+\sum_{i=2}^nH_i(X_i-X_{i-1})\]
is a (super/sub)martingale.
For a martingale, the condition $H_n\ge0$ is not required.
\end{parts}
\end{prb}


\begin{prb}[Martingale convergence theorems]
Let $(X_n)$ be a submartingale of random variables and let $a<b$.
Let $\tau^0<\tau_1<\tau^1<\tau_2<\cdots$ be a sequence of hitting times inductively defined by $\tau^0:=0$ and
\[\tau_k:=\min\{n>\tau^{k-1}:X_n\le a\},\qquad\tau^k:=\min\{n>\tau_k:X_n\ge b\},\qquad k\ge1.\]
Let $u_n:=\max\{k:\tau^k\le n\}$ be the number of upcrossing completed by time $n$.
\begin{parts}
\item We have
\[(b-a)Eu_n\le E(X_n-a)^+,\qquad n\ge1.\]
It is called the \emph{upcrossing inequality} by Doob.
\item If $\sup_nEX_n^+<\infty$, then $X_n$ converges a.s.~to a random variable $X$ such that $E|X|<\infty$.
\end{parts}
\end{prb}
\begin{pf}
(a)
Let $Y_n:=(X_n-a)^+$.
Note that $\tau^{u_n}\le n<\tau^{u_n+1}$.
Define a predictable sequence
\[H_n:=\sum_{k=1}^\infty\1_{(\tau_k,\tau^k]}(n)=\1_{\{\tau^{u_n}\}}(n)+\1_{(\tau_{u_n+1},\tau^{u_n+1})}(n).\]
Since $Y_{\tau_k}=0$ for any $k\ge1$, we have
\[(H\cdot Y)_n-(H\cdot Y)_{\tau^{u_n}}=\sum_{i=\tau^{u_n}+1}^nH_i(Y_i-Y_{i-1})=\1_{(\tau_{u_n+1},\tau^{u_n+1})}(n)\cdot(Y_n-Y_{\tau_{u_n+1}})\ge0,\]
so
\[(b-a)u_n=\sum_{k=1}^{u_n}(b-a)\le\sum_{k=1}^{u_n}(Y_{\tau^k}-Y_{\tau_k})=(H\cdot Y)_{\tau_{u_n}}\le(H\cdot Y)_n.\]
Since $(Y_n)$ is also a submartingale and $1-H_n\ge0$, we have
\[E((1-H)\cdot Y)_n\ge E((1-H)\cdot Y)_1=E((1-H_1)Y_1)\ge0,\]
hence
\[(b-a)Eu_n\le E(H\cdot Y)_n\le E(1\cdot Y)_n=EY_n-EY_1\le EY_n.\]

(b)
The condition $\sup_nEX_n^+<\infty$ implies that $\sup_nEu_n<\infty$ by the upcrossing inequality, so the increasing sequence $u_n$ converges a.s.
It means that
\[P\Bigl(\bigcup_{a,b\in\Q}\{\liminf_nX_n<a<b<\limsup_nX_n\}\Bigr)=0,\]
in other words, the limit $\lim_nX_n$ exists a.s.~in $[-\infty,\infty]$.
By the Fatou lemma,
\[E(\lim_n|X_n|)\le\liminf_nE|X_n|\le\liminf_n(2EX_n^+-EX_1)<\infty\]
implies $\lim_nX_n\in(-\infty,\infty)$ a.s.
\end{pf}



\begin{prb}[Doob inequality]
If $(X_n)$ is a non-negative submartingale, then we have the following Doob's (maximal or submartingale) inequality
\[P(X^*_n>\e)\le\frac1\e EX_n.\]
For $p>1$, if $\sup_nE|X_n|^p<\infty$, then $X_n$ converges a.s.~and in $L^p$.
\end{prb}




\begin{prb}[Uniform integrability]
We say a set of random variables $\{X_i\}$ is \emph{uniformly integrable} if
\[\lim_{c\to\infty}\sup_iE(|X_i|\1_{|X_i|>c})=0.\]
\end{prb}


\begin{prb}[Optional stopping theorem]
For a process $X$, the process \emph{stopped at} a random time $\tau$ is the process $X^\tau$ defined by $X^\tau_n:=X_{t\wedge\tau}$.
If $H_n:=\1_{n\le\tau}$, then $(H\cdot X)_n=X_{n\wedge\tau}$, so $X^\tau$ is a martingale if $X$ is martingale.
Wald equations
\end{prb}

\begin{prb}[Stopping time $\sigma$-algebra]***
Let $\tau$ be a stopping time, that is, $\{\tau\le t\}\in\cF_t$.
\[\cF_\tau:=\{A\in\cF:A\cap\{\tau\le t\}\in\cF_t\ \forall t\}.\]
\begin{parts}
\item $\cF_{\sigma\wedge\tau}=\cF_\sigma\cap\cF_\tau$.
\item D\'ebut theorem: for a \cadlag process $X$, the hitting time of a Borel set is a stopping time.
\item If $X$ is a uniformly integrable martingale and $\tau$ is a stopping time, then so is the $X^\tau$.
\end{parts}
\end{prb}
\begin{pf}
Since $\tau:\{\tau\le t\}\to\R_{\ge0}$ is measurable with respect to $\cF_t$, for all $x\in\R$ we have $\{X_\tau\le x\}\in\cF_\tau$.

(b)
Difficult.

(c)
By the optional stopping, we have $X^\tau_t=E(X_\tau|\cF_{t\wedge\tau})$, and in fact since $X_\tau1_{\{\tau\le t\}}$ is $\cF_t$-measurable, we further have
\begin{align*}
X^\tau_t&=E(X_\tau1_{\{\tau\le t\}}+X_\tau1_{\{\tau>t\}}|\cF_{t\wedge\tau})\\
&=X_\tau1_{\{\tau\le t\}}+E(X_\tau|\cF_{t\wedge\tau})1_{\{\tau>t\}}\\
&=X_\tau1_{\{\tau\le t\}}+E(X_\tau|\cF_t)1_{\{\tau>t\}}\\
&=E(X_\tau1_{\{\tau\le t\}}+X_\tau1_{\{\tau>t\}}|\cF_t)\\
&=E(X_\tau|\cF_t).
\end{align*}
So we are done.
\end{pf}



\section{Markov chains}


Random walks

Poisson process

Ornstein-Uhlenbeck

\section{Ergodic theory}



\section*{Exercises}



\chapter{Continuous stochastic processes}

\section{}
Kolmogorov extension
Poisson process, Wiener process, L\'evy process, Feller process, Markov process

Meyer's section theorems.


\section{Semi-martingales}

\begin{prb}[Filtrated proability space]
Let $(\Omega,(\cF_t),P)$ be a filtrated probability space with the usual condition.
We say a process $X$ is \emph{adapted} if $X_t\in\cF_t$ for all $t$.
A $\R_{\ge0}$-valued random variable $\tau$ is called a \emph{stopping time} if $\{\tau\le t\}\in\cF_t$ for all $t$.
\end{prb}


\begin{prb}[C\`adl\`ag modifications]
Two processes $X$ and $Y$ are called \emph{modifications} of each other if $X_t=Y_t$ a.s.~for each $t$, and \emph{indistinguishable} if $\sup_t|X_t-Y_t|=0$ a.s.

jump process $\Delta X$ defined such that $\Delta X_t:=X_t-X_{t-}$.
\begin{parts}
\item If $X$ and $Y$ are right continuous, then they are indistinguishable if and only if they are modifications.
\item For a submartingale $X$, $X$ has a \cadlag modification if and only if the non-decreasing function $t\mapsto EX_t$ is right continuous. The modification is unique. In particular, we can always assume a martingale is \cadlag because we assume the usual condition.
\end{parts}
\end{prb}

doob inequality and optional stopping


\begin{prb}[Local martingales]
A process $X$ is called a \emph{local martingale} if there is a sequence of stopping times $\tau_n$ such that $\tau_n\to\infty$ a.s.~and $X^{\tau_n}$ is a uniformly integrable martingale for each $n$.
When is a local martingale actually a martingale?
Kazamaki-Novikov criteria.
\end{prb}
"P. A. Meyer (1973) showed that there are no local martingales in discrete time; they are a continuous time phenomenon."

\begin{prb}[Semi-martingales]
\end{prb}




\begin{prb}[Doob-Meyer decomposition]
\end{prb}

Girsanov theorem




\section{Wiener spaces}
Cameron-Martin
centered Gaussian law
Ornstein-Uhlenbeck process
radonifying
martingale representation theorem, malliavin calculus




\part{Stochastic analysis}

\chapter{Stochastic integral}
\section{It\^o integral}
Stieltjes integral for locally bounded variation processes
square integrable martingale integral (by simple processes)
local martingale integral
Kunita-Watanabe inequality
Ito formula and semi-martingale

\section{Stratonovich integral}


\chapter{Stochastic differential equations}
\chapter{}



\part{Stochastic models}

phase transition, percolation


\end{document}
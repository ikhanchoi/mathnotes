\documentclass{../note}
\usepackage{../../ikany}

\def\Unif{\mathrm{Unif}}
\def\Bern{\mathrm{Bern}}
\def\Binom{\mathrm{Binom}}
\def\Geo{\mathrm{Geo}}
\def\Exp{\mathrm{Exp}}
\def\Pois{\mathrm{Pois}}



\begin{document}
\title{Probability Theory}
\author{Ikhan Choi}
\maketitle
\tableofcontents


\part{Probability distributions}


\chapter{Random variables}

\section{Sample spaces and distributions}
sample space of an "experiment"
random variables
distributions
expectation, moments, inequalities

equally likely outcomes
	coin toss
	dice roll
	ball drawing
	number permutation
	life time of a light bulb

joint distribution
transformation of distributions
distribution computations

\section{Discrete probability distributions}

\section{Continuous probability distributions}

\section{Independence}

\begin{prb}[Dynkin's $\pi$-$\lambda$ lemma]
Let $\cP$ be a $\pi$-system and $\cL$ a $\lambda$-system respectively.
Denote by $\ell(\cP)$ the smallest $\lambda$-system containing $\cP$.
\begin{parts}
\item If $A\in\ell(\cP)$, then $\cG_A:=\{B:A\cap B\in\ell(\cP)\}$ is a $\lambda$-system.
\item $\ell(\cP)$ is a $\pi$-system.
\item If a $\lambda$-system is a $\pi$-system, then it is a $\sigma$-algebra.
\item If $\cP\subset\cL$, then $\sigma(\cP)\subset\cL$.
\end{parts}
\end{prb}

\begin{prb}[Monotone class lemma]

\end{prb}







\chapter{Conditional probablity}

\begin{prb}[Monty Hall problem]
Suppose you're on a game show, and you're given the choice of three doors $A$, $B$, and $C$.
Behind one door is a car; behind the others, goats.
You pick a door, say $A$, and the host, who knows what's behind the doors, opens another door, say $B$, which has a goat.
He then says to you, ``Do you want to pick door $C$?''
Is it to your advantage to switch your choice?
\end{prb}
\begin{pf}
Let $A$, $B$, and $C$ be the events that a car is behind the doors $A$, $B$, and $C$, respectively.
Let $X$ be the event that the challenger picked $A$, and $Y$ the event that the game host opened $B$.
Note $\{A,B,C\}$ is a partition of the sample space $\Omega$, and $X$ is independent to $A$, $B$, and $C$.
Then, $P(A)=P(B)=P(C)=P(X)=1/3$, and
\[P(Y|X,A)=\frac12,\quad P(Y|X,B)=0,\quad P(Y|X,C)=1.\]
Therefore,
\begin{align*}
P(C|X,Y)&=\frac{P(X\cap Y\cap C)}{P(X\cap Y)}\\
&=\frac{P(Y|X,C)P(X\cap C)}{P(Y|X,A)P(X\cap A)+P(Y|X,B)P(X\cap B)+P(Y|X,C)P(X\cap C)}\\
&=\frac{1\cdot \frac19}{\frac12\cdot\frac19+0\cdot\frac19+1\cdot\frac19}=\frac23.
\end{align*}
Similarly, $P(A|X,Y)=\frac13$ and $P(B|X,Y)=0$.
\end{pf}





\chapter{}









\part{Limit theorems}



\chapter{Convergence of probability measures}


\section{Weak convergence in $\R$}
\begin{prb}[Portemanteau theorem]
Let $F_n$ and $F$ be distribution functions $\R\to[0,1]$.
We will define the \emph{weak convergence} as follows: $F_n$ converges weakly to $F$ if $F_n(x)\to F(x)$ for every continuity point $x$ of $F(x)$.
\begin{parts}
\item $F_n(x)\to F(x)$ for all continuity points $x$ of $F$.
\end{parts}
\end{prb}

\begin{prb}[Skorokhod representation theorem]
\end{prb}

\begin{prb}[Continuous mapping theorem]
\end{prb}

\begin{prb}[Slutsky's theorem]
\end{prb}

\begin{prb}[Helly's selection theorem]
\begin{parts}
\item Monotonically increasing functions $F_n:\R\to[0,1]$ has a pointwise convergent subsequence.
\item If $(F_n)_n$ is tight, then
\end{parts}
\end{prb}




\section{Weak convergence in metric spaces}

\begin{prb}
On metric spaces.
\begin{parts}
\item Every single measure is regular if $X$ is perfectly normal.
\item Every single measure is tight if $X$ is Polish.
\end{parts}
\end{prb}

% pi system that generates sigma alg is separating
% in \R^\infty and C(I), set of finite dimensional sets is separating


% convergence determining class
%  generalization of Portemanteau theorem
%  uniformly continuous functions: They can separate two closed sets like Urysohn.

\begin{prb}[Portemanteau theorem]
Let $\mu_n$ and $\mu$ be probability measures on a metric space $S$.
We will define the \emph{weak convergence} as follows: $\mu_n$ converges weakly to $\mu$ if
\[\int f\,d\mu_n\to\int f\,d\mu\]
for every $f\in C_b(S)$.
\begin{parts}
\item $\limsup_{n\to\infty}\mu_n(F)\le\mu(F)$ for all closed sets $F$.
\item $\liminf_{n\to\infty}\mu_n(G)\ge\mu(G)$ for all open sets $G$.
\end{parts}
\end{prb}
% 최소한 메트릭에서는 연속함수를 균등연속함수로 바꿀 수 있음


\begin{prb}[Skorokhod representation theorem]
\end{prb}

\begin{prb}[Continuous mapping theorem]
\end{prb}

\begin{prb}[Slutsky's theorem]
\end{prb}




\section{The space of probability measures}
\begin{prb}[Local limit theorems]
Suppose $f_n$ and $f$ are density functions.
\begin{parts}
\item If $f_n\to f$ a.s., then $f_n\to f$ in $L^1$.\hfill(Scheff\'e's theorem)
\item $f_n\to f$ in $L^1$ if and only if in total variation.
\item If $f_n\to f$ in total variation, then $f_n\to f$ weakly.
\end{parts}
\end{prb}

\begin{prb}[Vauge convergence]
Let $S$ be a locally compact Hausdorff space.
\begin{parts}
\item $\mu_n\to\mu$ vaguely if and only if $\int f\,d\mu_n\to\int f\,d\mu$ for all $f\in C_c(S)$.
\item $\mu_n\to\mu$ weakly if and only if vaguely.
\item $\delta_n\to0$ vaguely but not weakly. (escaping to infinity)
\end{parts}
\end{prb}
\begin{pf}
\end{pf}
% C_0가 convergence determining class라는 말

\begin{prb}[L\'evy-Prokhorov metric]
Let $S$ be a metric space, and $\Prob(S)$ be the set of probability Borel measures on $S$.
Define $\pi:\Prob(S)\times\Prob(S)\to[0,\infty)$ such that
\[\pi(\mu,\nu):=\inf\{\,\alpha>0:\mu(A)\le\nu(A^\alpha)+\alpha,\ \nu(A)\le\mu(A^\alpha)+\alpha,\ \forall A\in\cB(S)\,\},\]
where $A^\alpha$ is the $\alpha$-neighborhood of $a$.
\begin{parts}
\item $\pi$ is a metric.
\item $\mu_n\to\mu$ in $\pi$ implies $\mu_n\Rightarrow\mu$.
\item $\mu_\alpha\Rightarrow\mu$ implies $\mu_\alpha\to\mu$ in $\pi$, if $S$ is separable.
\item $(S,d)$ is separable if and only if $(\Prob(S),\pi)$ is separable.
\item $(S,d)$ is complete if and only if $(\Prob(S),\pi)$ is complete.
\end{parts}
\end{prb}
\begin{pf}
(c)
\end{pf}

\begin{prb}[Prokhorov's theorem]
Let $S$ be a metrizable space.
Let $\Prob(S)$ be the space of probability measures on $S$ endowed with the topology of weak convergence.
\begin{parts}
\item If $S$ is Polish, then the relative compactness implies the tightness.
\item The tightness implies the relative compactness.
\end{parts}
\end{prb}




\section{Characteristic functions}

\begin{prb}[Characteristic functions]
Let $\mu$ be a probability measure on $\R$.
Then, the \emph{characteristic function} of $\mu$ is defined by
\[\f(t):=Ee^{itX}=\int e^{itx}\,d\mu(x).\]
Note that $\f(t)=\hat\mu(-t)$ where $\hat\mu$ is the Fourier transform of $\mu\in\cS'(\R)$.
\begin{parts}
\item $\f\in C_b(\R)$.
\end{parts}
\end{prb}

\begin{prb}[Inversion formula]
Let $\mu$ be a probability measure on $\R$ and $\f$ its characteristic function.
\begin{parts}
\item For $a<b$, we have
\[\mu((a,b))+\frac12\mu(\{a,b\})=\lim_{T\to\infty}\frac1{2\pi}\int_{-T}^T\frac{e^{-ita}-e^{-itb}}{it}\f(t)\,dt.\]
\item For $a\in\R$, we have
\[\mu(\{a\})=\lim_{T\to\infty}\frac1{2T}\int_{-T}^Te^{-ita}\f(t)\,dt\]
\item If $\f\in L^1(\R)$, then $\mu$ has density
\[f(x)=\frac1{2\pi}\int e^{-itx}\f(t)\,dt\]
in $C_0(\R)\cap L^1(\R)$.
\end{parts}
\end{prb}

\begin{prb}[L\'evy's continuity theorem]
The continuity theorem provides with a tool to verify the weak convergence in terms of characteristic functions.
Let $\mu_n$ and $\mu$ be probability distributions on $\R$ with characteristic functions $\f_n$ and $\f$.
\begin{parts}
\item If $\mu_n\to\mu$ weakly, then $\f_n\to\f$ pointwise.
\item If $\f_n\to\f$ pointwise and $\f$ is continuous at zero, then $(\mu_n)_n$ is tight and $\mu_n\to\mu$ weakly.
\end{parts}
\end{prb}
\begin{pf}
(a)
For each $t$,
\[\f_n(t)=\int e^{itx}\,d\mu_n(x)\to\int e^{itx}\,d\mu(x)=\f(t)\]
because $e^{itx}\in C_b(\R)$.

(b)


\end{pf}

\begin{prb}[Criteria for characteristic functions]
Bochner's theorem and Polya's criterion
\end{prb}


There are two ways to represent a measure:
A measure $\mu$ is absolutely continuous iff its distribution $F$ is absolutely continuous iff its density $f$ is integrable.
So, the fourier transform of an absolutely continuous measure is just the fourier transform of $L^1$ functions.



\section{Moments}

moment problem

moment generating function defined on $|t|<\delta$


\section*{Exercises}
\begin{prb}
Let $\f_n$ be characteristic functions of probability measures $\mu_n$ on $\R$.
If there is a continuous function $\f$ such that $\f_n=\f$ on $n^{-1}\Z$, then $\mu_n$ converges weakly.
\end{prb}


















\chapter{Laws of large numbers}

\section{Weak and strong laws of large numbers}

Our purpose is to find appropriate $a_n$ and slowly growing $b_n$ such that $(S_n-a_n)/b_n\to0$ in probability or almost surely.


\begin{prb}[Truncation method]
Let $X_{n,i}:\Omega\to\R$ be uncorrelated random variables (with respect to $i$ for each $n$) and $S_n:=X_{n,1}+\cdots+X_{n,n}$.
For a positive sequence $(c_n)_{n=1}^\infty$, let $Y_{n,i}:=X_{n,i}\1_{|X_{n,i}|\le c_n}$ be truncated random variables and $T_n:=Y_{n,1}+\cdots+Y_{n,n}$.
Suppose that the truncation level $c_n$ satisfies the approximation condition
\[\lim_{n\to\infty}\sum_{i=1}^nP(|X_{n,i}|>c_n)=0.\]
\begin{parts}
\item If $(T_n-ET_n)/b_n\to0$ in probability, then $(S_n-ET_n)/b_n\to0$ in probability.
\item If $(T_n-ET_n)/b_n\to Z$ in distribution, then $(S_n-ET_n)/b_n\to Z$ in distribution.
\end{parts}
\end{prb}
\begin{pf}
(a)
Write
\[P\left(\left|\frac{S_n-ET_n}{b_n}\right|>\e\right)
\le P(S_n\ne T_n)+P\left(\left|\frac{T_n-ET_n}{b_n}\right|>\e\right)\to0\]
since
\[P(S_n\ne T_n)\le\sum_{i=1}^nP(X_{n,i}\ne Y_{n,i})=\sum_{i=1}^nP(|X_{n,i}|>c_n)\to0\]
as $n\to\infty$.

(b)
By the Slutsky theorem.
\end{pf}


\begin{prb}[Weak laws of large numbers]
Let $X_{n,i}:\Omega\to\R$ be uncorrelated random variables and $S_n:=X_{n,1}+\cdots+X_{n,n}$.
For a positive sequence $(c_n)_{n=1}^\infty$, let $Y_{n,i}:=X_{n,i}\1_{|X_{n,i}|\le c_n}$ be truncated random variables and $T_n:=Y_{n,1}+\cdots+Y_{n,n}$.

\begin{parts}
\item If
\[b_n^2\gg\sum_{i=1}^nE|Y_{n,i}-EY_{n,i}|^2,\]
then $(T_n-ET_n)/b_n\to0$ in probability.

\item Take slow $c_n$ as possible such that
\[1\gg\sum_{i=1}^nP(|X_{n,i}|>c_n).\]
Take slow $b_n$ as possible such that
\[b_n^2\gg\sum_{i=1}^nE|Y_{n,i}|^2.\]
then $(S_n-ET_n)/b_n\to0$.

\item If
\[\lim_{x\to\infty}\sup_ixP(|X_i|>x)=0,\]
then $(S_n-ET_n)/n\to0$ in probability.
This is called the \emph{Kolmogorov-Feller condition}.
\end{parts}
\end{prb}
\begin{pf}
(a)
Since $X_n$ are uncorrelated, we have for any $\e>0$ that
\[P\left(\left|\frac{S_n-ES_n}{c_n}\right|>\e\right)\le\frac1{\e^2c_n^2}VS_n\to0\]
as $n\to\infty$.

(c)
Write $g(x):=\sup_ixP(|X_i|>x)$.
Then, the truncation condition for $b_n=n$ is satisfied as
\[\sum_{i=1}^nP(|X_i|>n)\le\sum_{i=1}^n\frac1ng(n)=g(n)\to0\]
as $n\to\infty$.

On the other hand,
\begin{align*}
\frac1{n^2}\sum_{i=1}^nE|Y_i|^2
&=\frac1{n^2}\sum_{i=1}^n\int_0^\infty2xP(|Y_i|>x)\,dx
=\frac1{n^2}\sum_{i=1}^n\int_0^n2xP(|X_i|>x)\,dx\\
&\le\frac2n\int_0^ng(x)\,dx
=2\int_0^1g(nx)\,dx.
\end{align*}
Since $g(x)\le x$ and $g(x)\to0$ as $x\to\infty$, $g$ is bounded so that the bounded convergence theorem implies $\int_0^1g(nx)\,dx\to0$ as $n\to\infty$.

Therefore, $(T_n-ET_n)/n\to0$ in probability.
By the truncation
\end{pf}



\begin{prb}[Borel-Cantelli lemmas]
\end{prb}

\begin{prb}[Strong laws of large numbers]
Proof by Etemadi
\end{prb}

\section{Random series}

\section{Renewal theory}

\section*{Exercises}
\begin{prb}[Bernstein polynomial]
Let $X_n\sim\Bern(x)$ be i.i.d. random variables.
Since $S_n\sim\Binom(n,x)$, $E(S_n/n)=x$, $V(S_n/n)=x(1-x)/n$.
The $L^2$ law of large numbers implies $E(|S_n/n-x|^2)\to0$.
Define $f_n(x):=E(f(S_n/n))$.
Then, by the uniform continuity $|x-y|<\delta$ implies $|f(x)-f(y)|<\e$,
\[|f_n(x)-f(x)|\le E(|f(S_n/n)-f(x)|)\le\e+2\|f\|P(|S_n/n-x|\ge\delta)\to\e.\]
\end{prb}
\begin{prb}[High-dimensional cube is almost a sphere]
Let $X_n\sim\Unif(-1,1)$ be i.i.d. random variables and $Y_n:=X_n^2$.
Then, $E(Y_n)=\frac13$ and $V(Y_n)\le1$.
\end{prb}
\begin{prb}[Coupon collector's problem]
$T_n:=\inf\{\,t:|\{X_i\}_i|=n\,\}$
Since $X_{n,k}\sim\Geo(1-\frac{k-1}n)$, $E(X_{n,k})=(1-\frac{k-1}n)^{-1}$, $V(X_{n,k})\le(1-\frac{k-1}n)^{-2}$.
$E(T_n)\sim n\log n$
\end{prb}
\begin{prb}[An occupancy problem]
\end{prb}
\begin{prb}[The St. Petersburg paradox]
\end{prb}

\begin{prb}
Find the probability that arbitrarily chosen positive integers are coprime.
\end{prb}


\chapter{Central limit theorems}



\section{Central limit theorems}


\begin{prb}[Lyapunov central limit theorem]
Let $X_n:\Omega\to\R$ be independent random variables with $EX_i=\mu_i$ and $VX_i=\sigma_i^2$.
If there is $\delta>0$ such that the \emph{Lyapunov condition}
\[\lim_{n\to\infty}\frac1{s_n^{2+\delta}}\sum_{i=1}^nE|X_i-\mu_i|^{2+\delta}=0\]
is satisfied, then
\[\frac{S_n-ES_n}{s_n}\to N(0,1)\]
weakly, where $S_n:=\sum_{i=1}^nX_i$ and $s_n^2:=VS_n$.
\end{prb}

\begin{prb}[Lindeberg-Feller central limit theorem]
Let $X_{i,n}:\Omega\to\R$ be independent random variables and $S_n:=X_1+\cdots+X_n$.
\begin{parts}
\item
If
\[s_n^2\sim\sum_{i=1}^nE|X_i-EX_i|^2,\]
and if for every $\e>0$ we have
\[s_n^2\gg\sum_{i=1}^nE|X_i-EX_i|^2\1_{|X_i-EX_i|>\e s_n},\]
then $(S_n-ES_n)/s_n\to N(0,1)$ in distribution.
This is called the \emph{Lindeberg condition}.
\item

\end{parts}
\end{prb}


\section{Berry-Esseen ineaulity}



\section{Stable laws}


\section*{Exercises}

Poisson convergence, law of rare events, or weak law of small numbers (a single sample makes a significant attibution)














\part{Stochastic processes}
\chapter{Martingales}
\section{Submartingales}
\section{Martingale convergence theorem}
\begin{prb}[Doob's upcrossing inequality]
\begin{parts}
\item
\end{parts}
\end{prb}

\begin{prb}[Martingale convergence theorems]
\begin{parts}
\item
\end{parts}
\end{prb}

\begin{prb}
\begin{parts}
\item
\end{parts}
\end{prb}

\section{Convergence in $L^p$ and uniform integrability}

\section{Optional stopping theorem}





\chapter{Markov chains}
\chapter{Brownian motion}
\section{Kolomogorov extension}

\begin{prb}[Kolmogorov extension theorem]
A \emph{rectangle} is a finite product $\prod_{i=1}^nA_i\subset\R^n$ of measurable $A_i\subset\R$, and \emph{cylinder} is a product $A^*\times\R^\N$ where $A^*$ is a rectangle.
Let $\cA$ be the semi-algebra containing $\varnothing$ and all cylinders in $\R^\N$.
Let $(\mu_n)_n$ be a sequence of probability measures on $\R^n$ that satisfies \emph{consistency condition}
\[\mu_{n+1}(A^*\times\R)=\mu_n(A^*)\]
for any rectangles $A^*\subset\R^n$, and define a set function $\mu_0:\cA\to[0,\infty]$ by $\mu_0(A)=\mu_n(A^*)$ and $\mu_0(\varnothing)=0$.
\begin{parts}
\item $\mu_0$ is well-defined.
\item $\mu_0$ is finitely additive.
\item $\mu_0$ is countably additive if $\mu_0(B_n)\to0$ for cylinders $B_n\downarrow\varnothing$ as $n\to\infty$.
\item If $\mu_0(B_n)\ge\delta$, then we can find decreasing $D_n\subset B_n$ such that $\mu_0(D_n)\ge\frac\delta2$ and $D_n=D_n^*\times\R^\N$ for a compact rectangle $D_n^*$.
\item If $\mu_0(B_n)\ge\delta$, then $\bigcap_{i=1}^\infty B_i$ is non-empty.
\end{parts}
\end{prb}
\begin{pf}
(d)
Let $B_n=B_n^*\times\R^\N$ for a rectangle $B_n^*\subset\R^{r(n)}$.
By the inner regularity of $\mu_{r(n)}$, there is a compact rectangle $C_n^*\subset B_n^*$ such that
\[\mu_0(B_n\setminus C_n)=\mu_{r(n)}(B_n^*\setminus C_n^*)<\frac\delta{2^{n+1}}.\]
Let $C_n:=C_n^*\times\R^\N$ and define $D_n:=\bigcap_{i=1}^nC_i=D_n^*\times\R^\N$.
Then,
\[\mu_0(B_n\setminus D_n)\le\mu_0(\bigcup_{i=1}^nB_n\setminus C_i)\le\mu_0(\bigcup_{i=1}^nB_i\setminus C_i)<\frac\delta2,\]
which implies $\mu_0(D_n)\ge\frac\delta2$.

(e)
Take any sequence $(\omega_n)_n$ in $\R^\N$ such that $\omega_n\in D_n$.
Since each $D_n^*\subset\R^{r(n)}$ is compact and non-empty, by diagonal argument, we have a subsequence $(\omega_k)_k$ such that $\omega_k$ is pointwise convergent, and its limit is contained in $\bigcap_{i=1}^\infty D_i\subset\bigcap_{i=1}^\infty B_n=\varnothing$, which is a contradiction that leads $\mu_0(B_n)\to0$.
\end{pf}




\part{Stochastic calculus}

\end{document}